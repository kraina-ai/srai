{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Machine Learning model using Overture Maps data\n",
    "\n",
    "In this notebook, we aim to predict the distance to the nearest bicycle-sharing station in three Spanish cities: Madrid, Seville, and Valencia.\n",
    "\n",
    "Our goal is to develop a model capable of accurately predicting this distance based on geospatial features, which can assist in urban planning and enhance the accessibility of bicycle-sharing services.\n",
    "\n",
    "The task involves several key steps:\n",
    "\n",
    "1. **Loading bicycle-sharing stations**: \n",
    "We begin by loading geospatial data for bicycle-sharing stations from OpenStreetMap using [`OsmOnlineLoader`](../../loaders/osm_online_loader/) for the specified cities. We also visualise the locations of these stations.\n",
    "\n",
    "2. **Regionalization using H3 hexagons**:\n",
    "The cities are regionalized using H3 hexagons with [`H3Regionalizer`](../../regionalizers/h3_regionalizer/), enabling us to divide them into smaller, manageable regions. Next, we buffer the generated H3 regions and compute the distance from each hexagon to the nearest bicycle-sharing station.\n",
    "\n",
    "3. **Feature extraction and embedding**:\n",
    "We load additional geospatial features from Overture Maps using [`OvertureMapsLoader`](../../loaders/overture_maps_loader/) and join them with the H3 regions (with [`IntersectionJoiner`](../../joiners/intersection_joiner/)). We then generate embeddings for these regions using [`ContextualCountEmbedder`](../../embedders/contextual_count_embedder/), which will be used as input features for our machine learning model.\n",
    "\n",
    "4. **Training and evaluating the model**: \n",
    "We prepare the data for training and validation, standardize the features, and train an XGBoost regression model to predict the distance to the nearest bicycle-sharing station. Data from Madrid is used for training, while data from Seville is used for validation.\n",
    "\n",
    "5. **Prediction and Visualization**:\n",
    "Finally, the trained model predicts the distance to the nearest bicycle-sharing station for all three cities. The predicted distances and prediction errors are visualised on maps to evaluate the model's performance.\n",
    "\n",
    "<div class=\"admonition info\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "    <ul>\n",
    "        <li>12 GB of RAM</li>\n",
    "        <li>\n",
    "            Installed libraries: \n",
    "            <code>srai[osm,overturemaps,plotting]</code>, \n",
    "            <code>contextily</code>, <code>seaborn</code>, \n",
    "            <code>scikit-learn</code>, <code>xgboost</code>\n",
    "        </li>\n",
    "    </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/kraina-ai/srai/blob/main/examples/use_cases/simple_machine_learning_with_overture_maps_data.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to install the required packages\n",
    "# (e.g. when running in a new environment or Google Colab)\n",
    "\n",
    "# %pip install srai[osm,overturemaps,plotting] contextily seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from h3 import int_to_str, str_to_int\n",
    "from h3ronpy import grid_disk_aggregate_k\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from srai.embedders import ContextualCountEmbedder\n",
    "from srai.h3 import h3_to_shapely_geometry\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders import OSMOnlineLoader, OvertureMapsLoader\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "from srai.regionalizers import H3Regionalizer, geocode_to_region_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define regions of interest\n",
    "\n",
    "Geocode three Spanish cities using the [`geocode_to_region_gdf`](../../../api/regionalizers/#srai.regionalizers.geocode_to_region_gdf) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_names = [\"Madrid\", \"Seville\", \"Valencia\"]\n",
    "regions = geocode_to_region_gdf(cities_names)\n",
    "regions.index = cities_names\n",
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load bicycle-sharing stations data\n",
    "\n",
    "Load locations of the bicycle-sharing station from OpenStreetMap using  [`OsmOnlineLoader`](../../loaders/osm_online_loader/) for the defined regions.\n",
    "\n",
    "We will use the `{\"amenity\": \"bicycle_rental\"}` filter to get only locations where you can rental a bicycle (see the [`amenity=bicycle_rental`](https://wiki.openstreetmap.org/wiki/Tag:amenity%3Dbicycle_rental) definition on the OSM wiki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle_stations = OSMOnlineLoader().load(area=regions, tags={\"amenity\": \"bicycle_rental\"})\n",
    "bicycle_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the locations of the bicycle-sharing stations with defined cities geometries using [`IntersectionJoiner`](../../joiners/intersection_joiner/) to group them per city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle_stations_in_city = IntersectionJoiner().transform(\n",
    "    regions, bicycle_stations, return_geom=True\n",
    ")\n",
    "\n",
    "bicycle_stations_per_city = {}\n",
    "for city_name in cities_names:\n",
    "    bicycle_stations_per_city[city_name] = bicycle_stations_in_city.loc[city_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the Madrid data on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle_stations_per_city[\"Madrid\"].explore(tiles=\"CartoDB Positron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regionalize points data into H3 and create regions dataset\n",
    "\n",
    "Now we will transform the stations locations into the H3 cells using the [`H3Regionalizer`](../../regionalizers/h3_regionalizer/).\n",
    "\n",
    "After that, we will buffer those cells with grid disks together with aggregating the minimal distance to the station for each buffered H3 cell. We will use the [`grid_disk_aggregate_k`](https://h3ronpy.readthedocs.io/en/latest/usage/grid.html#grid-disk-aggregates-with-h3ronpy-grid-disk-aggregate-k) function from the `h3ronpy` library.\n",
    "\n",
    "Since `h3ronpy` operates on the Arrow tables and using integers as H3 indexes, we have to transform them from `str` to `int` and back again.\n",
    "\n",
    "We have to define 3 variables:\n",
    "- `H3_RESOLUTION`: What will be the H3 cell resolution?\n",
    "- `H3_PREDICTION_RANGE`: What is the maximum distance we aim to predict?\n",
    "- `H3_NEIGHBOURS`: How many neighbours do we want to have in the embedding context?\n",
    "\n",
    "We will then buffer the generated cells up to the `H3_PREDICTION_RANGE` + `H3_NEIGHBOURS` distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_RESOLUTION = 11\n",
    "H3_PREDICTION_RANGE = 10\n",
    "H3_NEIGHBOURS = 5\n",
    "\n",
    "\n",
    "def buffer_h3_cells_with_aggregation(h3_regions):\n",
    "    \"\"\"Expand H3 regions and calculate minimal distance to origin cells.\"\"\"\n",
    "    return (\n",
    "        pa.table(\n",
    "            grid_disk_aggregate_k(\n",
    "                h3_regions.index.map(str_to_int),\n",
    "                H3_NEIGHBOURS + H3_PREDICTION_RANGE,\n",
    "                \"min\",\n",
    "            )\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .rename(columns={\"k\": \"distance_to_station\", \"cell\": \"region_id\"})\n",
    "    )\n",
    "\n",
    "\n",
    "h3_regionalizer = H3Regionalizer(resolution=H3_RESOLUTION)\n",
    "h3_regions_gdfs = []\n",
    "for city_name, bicycle_stations_data in bicycle_stations_per_city.items():\n",
    "    city_h3_regions = h3_regionalizer.transform(bicycle_stations_data)\n",
    "\n",
    "    expanded_city_h3_regions = buffer_h3_cells_with_aggregation(city_h3_regions)\n",
    "    expanded_city_h3_regions[\"region_id\"] = expanded_city_h3_regions[\"region_id\"].map(int_to_str)\n",
    "    expanded_city_h3_regions = expanded_city_h3_regions.set_index(\"region_id\")\n",
    "    expanded_city_h3_regions[\"city\"] = city_name\n",
    "    expanded_city_h3_regions = gpd.GeoDataFrame(\n",
    "        expanded_city_h3_regions,\n",
    "        geometry=h3_to_shapely_geometry(expanded_city_h3_regions.index),\n",
    "        crs=4326,\n",
    "    )\n",
    "    h3_regions_gdfs.append(expanded_city_h3_regions)\n",
    "\n",
    "h3_regions = gpd.pd.concat(h3_regions_gdfs)\n",
    "\n",
    "h3_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll visualise the generated H3 regions.\n",
    "\n",
    "We will colour the cells based on the distance in the range defined in `H3_PREDICTION_RANGE` and we will colour the additional cells buffering the prediction range with additional neighbours (defined in the `H3_NEIGHBOURS` variable) in grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_cmap = LinearSegmentedColormap.from_list(\n",
    "    name=\"Temps\",\n",
    "    colors=[\n",
    "        \"#009392FF\",\n",
    "        \"#39B185FF\",\n",
    "        \"#9CCB86FF\",\n",
    "        \"#E9E29CFF\",\n",
    "        \"#EEB479FF\",\n",
    "        \"#E88471FF\",\n",
    "        \"#CF597EFF\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "ax = h3_regions[\n",
    "    (h3_regions.city == \"Madrid\") & (h3_regions.distance_to_station <= H3_PREDICTION_RANGE)\n",
    "].plot(\n",
    "    column=\"distance_to_station\",\n",
    "    figsize=(20, 20),\n",
    "    cmap=temps_cmap,\n",
    "    alpha=0.6,\n",
    "    legend=True,\n",
    "    legend_kwds={\n",
    "        \"shrink\": 0.3,\n",
    "        \"location\": \"bottom\",\n",
    "        \"label\": \"Distance to station\",\n",
    "        \"pad\": -0.05,\n",
    "    },\n",
    ")\n",
    "h3_regions[\n",
    "    (h3_regions.city == \"Madrid\") & (h3_regions.distance_to_station > H3_PREDICTION_RANGE)\n",
    "].plot(ax=ax, color=\"gray\", alpha=0.3)\n",
    "bicycle_stations_per_city[\"Madrid\"].representative_point().plot(ax=ax, color=\"black\", markersize=1)\n",
    "\n",
    "cx.add_basemap(ax, crs=h3_regions.crs, source=cx.providers.CartoDB.PositronNoLabels, zoom=13)\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Distance to the nearest bike station in Madrid\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare features for the model\n",
    "\n",
    "Now we will download the geospatial data for all of the H3 cells and generate embeddings for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Overture Maps data\n",
    "\n",
    "Now we will use [`OvertureMapsLoader`](../../loaders/overture_maps_loader/) to download the data for selected list of datasets defined by a theme/type pair. For each pair we will define a hierarchy depth used to aggregate features into columns.\n",
    "\n",
    "Data is downloaded using the [`OvertureMaestro`](https://kraina-ai.github.io/overturemaestro/latest/examples/advanced_functions/wide_form/) library.\n",
    "\n",
    "<div class=\"admonition info\">\n",
    "    <p class=\"admonition-title\">Switch to features based on OpenStreetMap</p>\n",
    "    <p>\n",
    "     If you want to use OpenStreetMap data instead you can use the <a href=\"../../loaders/osm_pbf_loader/\"><code>OSMPbfLoader</code></a> with <code>GEOFABRIK_LAYERS</code> filter.\n",
    "    </p>\n",
    "    <p>\n",
    "    <div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">srai.loaders.osm_loaders.filters</span> <span class=\"kn\">import</span> <span class=\"n\">GEOFABRIK_LAYERS</span>\n",
    "<span class=\"kn\">from</span> <span class=\"nn\">srai.loaders.osm_loaders</span> <span class=\"kn\">import</span> <span class=\"n\">OSMPbfLoader</span>\n",
    "\n",
    "<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">OSMPbfLoader</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">area</span><span class=\"o\">=</span><span class=\"n\">h3_regions</span><span class=\"p\">,</span> <span class=\"n\">tags</span><span class=\"o\">=</span><span class=\"n\">GEOFABRIK_LAYERS</span><span class=\"p\">)</span>\n",
    "</pre></div>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERTURE_MAPS_HIERARCHY_DEPTH_VALUES = {\n",
    "    (\"base\", \"infrastructure\"): 1,\n",
    "    (\"base\", \"land\"): 1,\n",
    "    (\"base\", \"land_use\"): 1,\n",
    "    (\"base\", \"water\"): 1,\n",
    "    (\"transportation\", \"segment\"): 2,\n",
    "    (\"buildings\", \"building\"): 2,\n",
    "    (\"places\", \"place\"): 1,\n",
    "}\n",
    "\n",
    "features = OvertureMapsLoader(\n",
    "    release=\"2024-12-18.0\",\n",
    "    theme_type_pairs=list(OVERTURE_MAPS_HIERARCHY_DEPTH_VALUES.keys()),\n",
    "    hierarchy_depth=list(OVERTURE_MAPS_HIERARCHY_DEPTH_VALUES.values()),\n",
    "    include_all_possible_columns=False,\n",
    ").load(area=h3_regions)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersect downloaded features with H3 regions\n",
    "\n",
    "Now we will join geometries of each region and feature using [`STRTree`](https://geopandas.org/en/stable/docs/reference/sindex.html) spatial index from the `GeoPandas` library.\n",
    "\n",
    "As a result, we will get a multiindex with `region_id` and `feature_id` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = IntersectionJoiner().transform(regions=h3_regions, features=features)\n",
    "joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding\n",
    "\n",
    "The simplest possible embedding in geospatial context is the number of features per category in a given region. You can generate it using [`CountEmbedder`](../../embedders/count_embedder/) and it is the equivalent of [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from the `scikit-learn` library.\n",
    "\n",
    "Here we will use a slightly more advanced method, taking into account close proximity in geographical space, so that each region also has information about what is in its neighbourhood. We call it `Context` and we will use the [`ContextualCountEmbedder`](../../embedders/contextual_count_embedder/) to get this kind of embedding.\n",
    "\n",
    "This method will retrieve information about neighbours at a distance less than or equal to the variable `H3_NEIGHBOURS` and add up the number of objects in each ring. Then those aggregated rings could be added as additional columns (`concatenate_vectors`=`True`) or added to existing columns using diminishing weight based on the distance from the central region (`concatenate_vectors`=`False`, default).\n",
    "\n",
    "We also have to pass a `Neighbourhood` object, that will be used to calculate adjacency between regions. Because we are working with H3 cells, we can pass [`H3Neighbourhood`](../../neighbourhoods/h3_neighbourhood/) object to utilize fast internal `H3` methods used to calculate neighbourhood of an H3 cell, instead of relying on slower geometry-based operations.\n",
    "\n",
    "Since the data from the `OvertureMapsLoader` is returned in the form of `bool` columns (instead of default `str` columns with `None` values), the have to change default value of the `count_subcategories` from `True` to `False`. All of the subcategories are already present in the final form of features `GeoDataFrame`.\n",
    "\n",
    "<div class=\"admonition warning\">\n",
    "    <p class=\"admonition-title\">Using OpenStreetMap features</p>\n",
    "    <p>\n",
    "     If you are using the OpenStreetMap data, then change the <code>count_subcategories</code> parameter to <code>True</code>.\n",
    "    </p>\n",
    "    <hr>\n",
    "    <p>\n",
    "     Also, remember to remove bicycle sharing stations from the dataset (<code>amenity=bicycle_sharing</code>), since we don't want it to be present in the final dataset.\n",
    "    </p>\n",
    "    <p>\n",
    "     Overture Maps doesn't have any bicycle sharing stations data in the <code>2024-12-18.0</code> release, so we don't need to do this when using Overture Maps data.\n",
    "    </p>\n",
    "    <p>\n",
    "    <div class=\"highlight\"><pre><span></span><span class=\"n\">embeddings</span> <span class=\"o\">=</span> <span class=\"n\">embeddings</span><span class=\"o\">.</span><span class=\"n\">drop</span><span class=\"p\">(</span>\n",
    "    <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span>\n",
    "        <span class=\"n\">c</span> <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">embeddings</span><span class=\"o\">.</span><span class=\"n\">columns</span>\n",
    "        <span class=\"k\">if</span> <span class=\"s2\">\"bicycle_rental\"</span> <span class=\"ow\">in</span> <span class=\"n\">c</span>\n",
    "    <span class=\"p\">]</span>\n",
    "<span class=\"p\">)</span>\n",
    "</pre></div>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ContextualCountEmbedder(\n",
    "    neighbourhood=H3Neighbourhood(),\n",
    "    neighbourhood_distance=H3_NEIGHBOURS,\n",
    "    concatenate_vectors=False,\n",
    "    count_subcategories=False,\n",
    ").transform(regions_gdf=h3_regions, features_gdf=features, joint_gdf=joint)\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "After preparing the features for all three cities at once, it is now time to split them, prepare them for training and train the `XGBoost` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the target and cities for training\n",
    "\n",
    "We will now define the target column that will be used for the model to predict and select train and validation cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"distance_to_station\"\n",
    "TRAIN_CITY = \"Madrid\"\n",
    "VALIDATION_CITY = \"Seville\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the cities datasets with generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "madrid_data = h3_regions[h3_regions[\"city\"] == \"Madrid\"].merge(\n",
    "    embeddings, left_index=True, right_index=True\n",
    ")\n",
    "\n",
    "seville_data = h3_regions[h3_regions[\"city\"] == \"Seville\"].merge(\n",
    "    embeddings, left_index=True, right_index=True\n",
    ")\n",
    "\n",
    "valencia_data = h3_regions[h3_regions[\"city\"] == \"Valencia\"].merge(\n",
    "    embeddings, left_index=True, right_index=True\n",
    ")\n",
    "\n",
    "\n",
    "madrid_data.shape, seville_data.shape, valencia_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "madrid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the features\n",
    "\n",
    "Each city might have a different density of features and we don't want to be sensitive to those changes. \n",
    "\n",
    "Let's consider a scenario where in a city A, there is a lot of benches, and a city B where there isn't a lot of benches. Now, there would naturally be a notable difference in the number of benches we can expect in a given region in a city A vs city B.\n",
    "Scaling the features for each of those cities will give us a relative information about how many benches there are in a given region in regards to a particular city. And that's why we want to scale the features, to keep different cities in a similar domain, so that the transfer of knowledge between them is easier for the model. \n",
    "\n",
    "Here we will use a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from the `scikit-learn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_madrid = StandardScaler().fit_transform(madrid_data[embeddings.columns])\n",
    "y_madrid = madrid_data[TARGET]\n",
    "\n",
    "x_seville = StandardScaler().fit_transform(seville_data[embeddings.columns])\n",
    "y_seville = seville_data[TARGET]\n",
    "\n",
    "x_valencia = StandardScaler().fit_transform(valencia_data[embeddings.columns])\n",
    "y_valencia = valencia_data[TARGET]\n",
    "\n",
    "x_madrid.shape, y_madrid.shape, x_seville.shape, y_seville.shape, x_valencia.shape, y_valencia.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "It is finally time to train the model.\n",
    "\n",
    "We will use the [`XGBoost`](https://xgboost.readthedocs.io/en/stable/) library as a popular choice in the data science domain.\n",
    "\n",
    "We will create two datasets: train and validation and we will set some parameters for the booster training.\n",
    "\n",
    "Our loss function will be Root Mean Squared Error (RMSE) and we will use early stopping to avoid overfitting to a single city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y_madrid <= H3_PREDICTION_RANGE\n",
    "dtrain = xgb.DMatrix(\n",
    "    data=x_madrid[mask], label=y_madrid[mask], feature_names=list(embeddings.columns)\n",
    ")\n",
    "mask = y_seville <= H3_PREDICTION_RANGE\n",
    "dval = xgb.DMatrix(\n",
    "    data=x_seville[mask], label=y_seville[mask], feature_names=list(embeddings.columns)\n",
    ")\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eta\": 0.01,\n",
    "    \"max_depth\": 8,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    verbose_eval=50,\n",
    "    early_stopping_rounds=100,\n",
    "    evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.best_iteration, bst.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the results\n",
    "\n",
    "Now we will evaluate the model and visualise the results on a map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the distance for all cities\n",
    "\n",
    "First we need to predict the distance for all three cities to have a set to compare to.\n",
    "\n",
    "We will calculate the prediction error using predicted value and expected distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_regions = pd.concat([madrid_data, seville_data, valencia_data])[\n",
    "    [TARGET, \"city\", \"geometry\"]\n",
    "]\n",
    "\n",
    "concatenated_regions[\"predicted_distance_to_station\"] = (\n",
    "    np.concatenate(\n",
    "        [\n",
    "            bst.predict(xgb.DMatrix(x_madrid, feature_names=list(embeddings.columns))),\n",
    "            bst.predict(xgb.DMatrix(x_seville, feature_names=list(embeddings.columns))),\n",
    "            bst.predict(xgb.DMatrix(x_valencia, feature_names=list(embeddings.columns))),\n",
    "        ]\n",
    "    )\n",
    "    .round(2)\n",
    "    .clip(min=0)\n",
    ")\n",
    "\n",
    "concatenated_regions = concatenated_regions[concatenated_regions[TARGET] <= H3_PREDICTION_RANGE]\n",
    "\n",
    "concatenated_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the distribution of predictions\n",
    "\n",
    "First, we will see how the model performed in terms of distance prediction by comparing the predicted values with the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal12_cmap = ListedColormap(\n",
    "    name=\"pal12\",\n",
    "    colors=[\n",
    "        \"#DE1A1AFF\",\n",
    "        \"#E1581AFF\",\n",
    "        \"#E37F1BFF\",\n",
    "        \"#E2A11BFF\",\n",
    "        \"#DFC11BFF\",\n",
    "        \"#D8E01BFF\",\n",
    "        \"#CEFF1AFF\",\n",
    "        \"#B2D736FF\",\n",
    "        \"#97B043FF\",\n",
    "        \"#7B8948FF\",\n",
    "        \"#5F654AFF\",\n",
    "        \"#43444AFF\",\n",
    "        \"#202547FF\",\n",
    "    ][: (H3_PREDICTION_RANGE + 1)],\n",
    ")\n",
    "\n",
    "_, axs = plt.subplots(2, 3, figsize=(12, 8), sharey=True, sharex=True, dpi=600)\n",
    "\n",
    "axs[0, 0].set_ylabel(\"Predicted distance to station\")\n",
    "axs[1, 0].set_ylabel(\"Predicted distance to station\")\n",
    "\n",
    "for idx, city_name in enumerate(cities_names):\n",
    "    expected_values = concatenated_regions[concatenated_regions[\"city\"] == city_name][TARGET]\n",
    "    mask = expected_values <= H3_PREDICTION_RANGE\n",
    "    expected_values = expected_values[mask]\n",
    "    predicted_values = concatenated_regions[concatenated_regions[\"city\"] == city_name][\n",
    "        \"predicted_distance_to_station\"\n",
    "    ][mask]\n",
    "\n",
    "    sns.regplot(\n",
    "        x=expected_values,\n",
    "        y=predicted_values,\n",
    "        ax=axs[0, idx],\n",
    "        scatter=True,\n",
    "        order=2,\n",
    "        scatter_kws=dict(\n",
    "            alpha=0.02,\n",
    "            color=[pal12_cmap.colors[_y] for _y in expected_values],\n",
    "        ),\n",
    "        line_kws=dict(\n",
    "            color=\"black\",\n",
    "        ),\n",
    "        x_jitter=0.1,\n",
    "    )\n",
    "    sns.violinplot(\n",
    "        x=expected_values,\n",
    "        y=predicted_values,\n",
    "        ax=axs[1, idx],\n",
    "        fill=True,\n",
    "        palette=pal12_cmap.colors,\n",
    "        hue=expected_values,\n",
    "        legend=False,\n",
    "    )\n",
    "    title = city_name\n",
    "    if city_name == TRAIN_CITY:\n",
    "        title += \" (train)\"\n",
    "    elif city_name == VALIDATION_CITY:\n",
    "        title += \" (validation)\"\n",
    "\n",
    "    axs[0, idx].set_title(title)\n",
    "    axs[0, idx].set_xlabel(None)\n",
    "    axs[1, idx].set_xlabel(\"Distance to station\")\n",
    "\n",
    "axs[0, 0].set_ylim(bottom=0)\n",
    "axs[1, 0].set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's far from perfect, but we can see that the overall trend from the lowest to the highest distance is preserved for both validation city (Seville) and the city that didn't take part in the modelling (Valencia).\n",
    "\n",
    "Although the scale isn't preserved and errors are significant, we can still use those predictions to display some hotspots on a map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the predictions on a map\n",
    "\n",
    "Here comes the most interesting part - visualising the predictions on a map.\n",
    "\n",
    "We will change the scale for each of the cities to mach the scale of predictions, so that we can clearly see the difference between lowest and highest predictions.\n",
    "\n",
    "Existing bicycle stations will be plotted as black dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city_name in cities_names:\n",
    "    city_data = concatenated_regions[concatenated_regions.city == city_name]\n",
    "    ax = city_data.plot(\n",
    "        column=\"predicted_distance_to_station\",\n",
    "        figsize=(20, 20),\n",
    "        cmap=temps_cmap,\n",
    "        alpha=0.8,\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "            \"shrink\": 0.3,\n",
    "            \"location\": \"bottom\",\n",
    "            \"label\": \"Predicted distance to station\",\n",
    "            \"pad\": -0.05,\n",
    "        },\n",
    "        vmin=max(0, city_data[\"predicted_distance_to_station\"].min()),\n",
    "        vmax=city_data[\"predicted_distance_to_station\"].max(),\n",
    "    )\n",
    "    bicycle_stations_per_city[city_name].representative_point().plot(\n",
    "        ax=ax, color=\"black\", markersize=3, alpha=0.4\n",
    "    )\n",
    "\n",
    "    cx.add_basemap(ax, crs=h3_regions.crs, source=cx.providers.CartoDB.PositronNoLabels, zoom=13)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    title = f\"Predicted distance to the nearest bike station in {city_name}\"\n",
    "    if city_name == TRAIN_CITY:\n",
    "        title += \" (train)\"\n",
    "    elif city_name == VALIDATION_CITY:\n",
    "        title += \" (validation)\"\n",
    "\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the predictions error on a map\n",
    "\n",
    "And the last part, the prediction error preview.\n",
    "\n",
    "As in other areas of data science, we can make a boring error graph on the X / Y axis or as a histogram. But what would the field of geospatial data be if we didn't also use maps for this.\n",
    "\n",
    "Since we already know that ranges in the predictions per city aren't matched with the original domain, we will rescale the predictions to be in the original domain and recalculate the prediction error. That way we will be able to eliminate the base error resulting from the difference between scales and focus on the spatial part of the error.\n",
    "\n",
    "We will also normalize the prediction error, to the 0-1 scale so that the biggest negative error will correlate with the value of 0, biggest positive error to the value of 1, and value of 0.5 will represent no error at all.\n",
    "\n",
    "That way we will have an easy way to utilize divergent colour scale without rescaling it.\n",
    "\n",
    "Additionally, we will use opacity to hide regions where the error was insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangerine_blues_cmap = LinearSegmentedColormap.from_list(\n",
    "    name=\"TangerineBlues\",\n",
    "    colors=[\n",
    "        \"#552000FF\",\n",
    "        \"#8A4D00FF\",\n",
    "        \"#C17D17FF\",\n",
    "        \"#F8B150FF\",\n",
    "        \"#F5F5F5FF\",\n",
    "        \"#93C6E1FF\",\n",
    "        \"#5F93ACFF\",\n",
    "        \"#2E627AFF\",\n",
    "        \"#00344AFF\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "for city_name in cities_names:\n",
    "    city_data = concatenated_regions[concatenated_regions.city == city_name].copy()\n",
    "\n",
    "    # NewValue = (((OldValue - OldMin) * (NewMax - NewMin)) / (OldMax - OldMin)) + NewMin\n",
    "    city_data[\"scaled_predicted_distance_to_station\"] = (\n",
    "        (\n",
    "            (\n",
    "                city_data[\"predicted_distance_to_station\"]\n",
    "                - city_data[\"predicted_distance_to_station\"].min()\n",
    "            )\n",
    "            * (city_data[\"distance_to_station\"].max() - city_data[\"distance_to_station\"].min())\n",
    "        )\n",
    "        / (\n",
    "            city_data[\"predicted_distance_to_station\"].max()\n",
    "            - city_data[\"predicted_distance_to_station\"].min()\n",
    "        )\n",
    "    ) + city_data[\"distance_to_station\"].min()\n",
    "\n",
    "    city_data[\"scaled_prediction_error\"] = (\n",
    "        city_data[TARGET].clip(upper=H3_PREDICTION_RANGE)\n",
    "        - city_data[\"scaled_predicted_distance_to_station\"]\n",
    "    )\n",
    "\n",
    "    city_data[\"normalized_scaled_prediction_error\"] = (\n",
    "        city_data[\"scaled_prediction_error\"].apply(\n",
    "            lambda x, city_data=city_data: (\n",
    "                -x / city_data[\"scaled_prediction_error\"].min()\n",
    "                if x < 0\n",
    "                else x / city_data[\"scaled_prediction_error\"].max()\n",
    "            )\n",
    "        )\n",
    "        + 1\n",
    "    ) / 2\n",
    "\n",
    "    city_data[\"normalized_prediction_error_alpha\"] = (\n",
    "        city_data[\"normalized_scaled_prediction_error\"] - 0.5\n",
    "    ).abs() * 2\n",
    "\n",
    "    ax = city_data.plot(\n",
    "        column=\"normalized_scaled_prediction_error\",\n",
    "        figsize=(20, 20),\n",
    "        cmap=tangerine_blues_cmap,\n",
    "        alpha=city_data[\"normalized_prediction_error_alpha\"],\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "            \"shrink\": 0.3,\n",
    "            \"location\": \"bottom\",\n",
    "            \"label\": \"Distance prediction error (scaled)\",\n",
    "            \"pad\": -0.05,\n",
    "            \"ticks\": [0, 0.5, 1],\n",
    "            \"format\": mticker.FixedFormatter(\n",
    "                [\n",
    "                    round(city_data[\"scaled_prediction_error\"].min(), 2),\n",
    "                    \"0\",\n",
    "                    round(city_data[\"scaled_prediction_error\"].max(), 2),\n",
    "                ]\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    bicycle_stations_per_city[city_name].representative_point().plot(\n",
    "        ax=ax, color=\"black\", markersize=3, alpha=0.4\n",
    "    )\n",
    "\n",
    "    cx.add_basemap(ax, crs=h3_regions.crs, source=cx.providers.CartoDB.PositronNoLabels, zoom=13)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    title = f\"Distance prediction error (scaled) in {city_name}\"\n",
    "    if city_name == TRAIN_CITY:\n",
    "        title += \" (train)\"\n",
    "    elif city_name == VALIDATION_CITY:\n",
    "        title += \" (validation)\"\n",
    "\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of the scaling we have done in a previous step.\n",
    "\n",
    "The difference between predicted distances and predicted distances scaled to the original domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    data=pd.melt(\n",
    "        city_data,\n",
    "        value_vars=[\n",
    "            \"predicted_distance_to_station\",\n",
    "            \"scaled_predicted_distance_to_station\",\n",
    "        ],\n",
    "    ),\n",
    "    x=\"value\",\n",
    "    hue=\"variable\",\n",
    "    kde=True,\n",
    "    height=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See feature importance \n",
    "\n",
    "We can visualise feature importance from the `XGBoost` Booster object and display the gain per feature. This info can tell us, or the planners, what urban features are important for the model. \n",
    "\n",
    "This is a basic form of explainability and it doesn't tell us if the feature has negative or positive impact on the result. For that we would have to use for example the SHAP library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    bst.get_score(importance_type=\"gain\").items(), columns=[\"Feature\", \"Gain\"]\n",
    ")\n",
    "ax = sns.barplot(y=\"Feature\", x=\"Gain\", data=feature_importance_df.nlargest(20, columns=\"Gain\"))\n",
    "ax.set_title(\"Feature importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In summary, looking at the prediction maps, and the errors, there is a general trend where the predictions fairly represent the true layout of the cycle stations.\n",
    "\n",
    "There is a strong predominance of city centres, where these predictions are closer to reality, and the model tends to predict low values there. As you move away from the centre, you can see increasing deviations, with noticeable clusters where the model completely failed to predict the existence of a cycle station.\n",
    "\n",
    "This may be due to a number of factors:\n",
    "- differences in planning of the cycling network between towns and cities;\n",
    "- differences in urban layout of towns and cities;\n",
    "- lack of features or examples representing some types of urban tissue;\n",
    "- model unable to catch all the complexities of the data set;\n",
    "- too simple method of embedding geographic features into the numerical vector.\n",
    "\n",
    "Even looking at the example of Madrid, which has been reproduced quite well by the model, you can see examples where the model thinks a station should not be there, or has found empty places where it would propose to put a station, so even without knowledge transfer between cities, such a model could support local planners.\n",
    "\n",
    "---\n",
    "\n",
    "I hope that the methods presented here have helped you understand the basics of the field of geospatial machine learning and presented the tangible benefits of using such a model in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
