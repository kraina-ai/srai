{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from collections.abc import Iterator\n",
    "from typing import Any, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from srai.benchmark import TrajectoryRegressionEvaluator\n",
    "from srai.datasets import PortoTaxiDataset\n",
    "from srai.embedders import Hex2VecEmbedder  # noqa: F401\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders.osm_loaders import OSMPbfLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.regionalizers import H3Regionalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resolution = 9\n",
    "embedder_hidden_sizes = [150, 75, 25]\n",
    "regionalizer = H3Regionalizer(resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi = PortoTaxiDataset()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = porto_taxi.load(hf_token=hf_token, version=\"TTE\")\n",
    "train, test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sample(frac=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dev split from train split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = porto_taxi.train_test_split_bucket_trajectory(\n",
    "    trajectory_id_column=\"trip_id\", task=\"TTE\", test_size=0.1, bucket_number=7, dev=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(porto_taxi.dev_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac=0.2, random_state=42)\n",
    "dev = dev.sample(frac=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi.categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi.numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linestring embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = train.copy()\n",
    "dev_ = dev.copy()\n",
    "test_ = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train = regionalizer.transform(train_)\n",
    "regions_dev = regionalizer.transform(dev_)\n",
    "regions_test = regionalizer.transform(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_features = OSMPbfLoader().load(regions_train, HEX2VEC_FILTER)\n",
    "embedder = Hex2VecEmbedder(embedder_hidden_sizes)\n",
    "region_intersect_train = IntersectionJoiner().transform(regions_train, osm_features)\n",
    "neighbourhood = H3Neighbourhood(regions_train)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    embedder.fit(\n",
    "        regions_gdf=regions_train,\n",
    "        features_gdf=osm_features,\n",
    "        joint_gdf=region_intersect_train,\n",
    "        neighbourhood=neighbourhood,\n",
    "        trainer_kwargs={\"max_epochs\": 10, \"accelerator\": device},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = embedder.transform(\n",
    "    regions_gdf=regions_train, features_gdf=osm_features, joint_gdf=region_intersect_train\n",
    ")\n",
    "embeddings_train[\"h3\"] = embeddings_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_features_dev = OSMPbfLoader().load(regions_dev, HEX2VEC_FILTER)\n",
    "osm_features_test = OSMPbfLoader().load(regions_test, HEX2VEC_FILTER)\n",
    "\n",
    "region_intersect_dev = IntersectionJoiner().transform(regions_dev, osm_features_dev)\n",
    "region_intersect_test = IntersectionJoiner().transform(regions_test, osm_features_test)\n",
    "\n",
    "embeddings_dev = embedder.transform(\n",
    "    regions_gdf=regions_dev, features_gdf=osm_features_dev, joint_gdf=region_intersect_dev\n",
    ")\n",
    "embeddings_test = embedder.transform(\n",
    "    regions_gdf=regions_test, features_gdf=osm_features_test, joint_gdf=region_intersect_test\n",
    ")\n",
    "embeddings_test[\"h3\"] = embeddings_test.index\n",
    "embeddings_dev[\"h3\"] = embeddings_dev.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns(row: gpd.GeoSeries) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate embedding values together.\n",
    "\n",
    "    Args:\n",
    "        row (gpd.GeoSeries): row of embeddings\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: concatenated embedding\n",
    "    \"\"\"\n",
    "    return np.concatenate([np.atleast_1d(float(val)) for val in row.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = embeddings_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "embeddings_train[\"embedding\"] = embeddings_train[numeric_cols].apply(concat_columns, axis=1)\n",
    "embeddings_dev[\"embedding\"] = embeddings_dev[numeric_cols].apply(concat_columns, axis=1)\n",
    "embeddings_test[\"embedding\"] = embeddings_test[numeric_cols].apply(concat_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_embeddings_to_trips(traj_df, embedding_df):\n",
    "    \"\"\"\n",
    "    Adds a column to traj_df with a list of embedding vectors matching the h3 sequence.\n",
    "\n",
    "    Args:\n",
    "        traj_df (pd.DataFrame): A DataFrame with columns [\"trip_id\", \"duration\", \"h3_sequence\"].\n",
    "        embedding_df (pd.DataFrame): Index is h3 index, values are embedding vectors.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: New DataFrame with an additional 'embedding_sequence' column.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding_sequence(h3_seq):\n",
    "        embeddings = []\n",
    "        for h in h3_seq:\n",
    "            if h in embedding_df.index:\n",
    "                emb = embedding_df.loc[h]\n",
    "                # If the embedding is a Series, convert to numpy array\n",
    "                if hasattr(emb, \"values\"):\n",
    "                    emb = emb.values\n",
    "                embeddings.append(emb)\n",
    "            else:\n",
    "                # Handle missing h3 (e.g., pad with zeros or skip)\n",
    "                embeddings.append(np.zeros(embedding_df.shape[1]))\n",
    "        return embeddings\n",
    "\n",
    "    traj_df = traj_df.copy()\n",
    "    traj_df[\"embedding_sequence\"] = traj_df[\"h3_sequence\"].apply(get_embedding_sequence)\n",
    "    return traj_df[[\"trip_id\", \"duration\", \"h3_sequence\", \"embedding_sequence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = attach_embeddings_to_trips(train, embeddings_train[\"embedding\"])\n",
    "merged_dev = attach_embeddings_to_trips(dev, embeddings_dev[\"embedding\"])\n",
    "merged_test = attach_embeddings_to_trips(test, embeddings_test[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(df: pd.DataFrame) -> Iterator[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generator function to yield training examples from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing columns:\n",
    "            - \"embedding_sequence\": A list of embedding vectors for each H3 hex in the trajectory\n",
    "              (i.e., List[List[float]] or numpy.ndarray of shape (seq_len, embed_dim)).\n",
    "            - \"trip_id\": Unique identifier for each trip (e.g., a string or integer).\n",
    "            - \"duration\": Target variable for the trip duration (e.g., a float or int).\n",
    "\n",
    "    Yields:\n",
    "        Dict[str, Any]: A dictionary with keys:\n",
    "            - \"X\": The embedding sequence representing the trajectory.\n",
    "            - \"trip_id\": The unique trip identifier.\n",
    "            - \"y\": The target duration for the trip.\n",
    "    \"\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        yield {\n",
    "            \"X\": row[\"embedding_sequence\"],  # shape: (seq_len, embed_dim), as list\n",
    "            \"trip_id\": row[\"trip_id\"],  # list of h3 indexes\n",
    "            \"y\": row[\"duration\"],  # target\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_generator(lambda: generate_examples(merged_train))\n",
    "dev_dataset = Dataset.from_generator(lambda: generate_examples(merged_dev))\n",
    "test_dataset = Dataset.from_generator(lambda: generate_examples(merged_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trajectory model module.\n",
    "\n",
    "This module contains implementation of base model of trajectory.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TravelTimePredictionBaseModel(nn.Module):  # type: ignore\n",
    "    \"\"\"\n",
    "    Travel time prediction base model.\n",
    "\n",
    "    Definition of travel time prediction model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int):\n",
    "        \"\"\"\n",
    "        Initialization of travel time prediction module.\n",
    "\n",
    "        Args:\n",
    "            input_size: number of input features\n",
    "            hidden_size:  number of features in the hidden state of the LSTM\n",
    "            num_layers: The number of recurrent layers in the LSTM\n",
    "            output_size: number of output features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Padded input tensor of shape (batch_size, seq_len, input_size), \\\n",
    "                where `seq_len` is the maximum sequence length in the batch and `input_size` \\\n",
    "                is the dimensionality of each timestep's feature vector (e.g., embedding size).\n",
    "            lengths (torch.Tensor): 1D tensor of shape (batch_size,) containing the original \\\n",
    "                (unpadded) lengths of each sequence in the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_size), containing the \\\n",
    "                predicted values for each sequence in the batch.\n",
    "        \"\"\"\n",
    "        # Handling varying length of sequences\n",
    "        packed_input = pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # hn: (num_layers, batch_size, hidden_size)\n",
    "        # We can use the last layer's hidden state for regression\n",
    "        final_hidden = hn[-1]  # (batch_size, hidden_size)\n",
    "\n",
    "        out = self.fc(final_hidden)\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sample_input = merged_train[\"embedding_sequence\"].iloc[0]\n",
    "# len of single h3 embedding\n",
    "input_size = sample_input[0].shape[0]\n",
    "output_size = 1  # Predicting total duration (regression)\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "model = TravelTimePredictionBaseModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    ")\n",
    "evaluator = TrajectoryRegressionEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to handle variable-length sequences.\n",
    "\n",
    "    Pads a batch of embedding sequences to the maximum sequence length in the batch,\n",
    "    and prepares corresponding labels and metadata for model input.\n",
    "\n",
    "    Args:\n",
    "        batch (List[dict[str, Any]]): A list of examples, where each example is a dictionary\n",
    "            containing:\n",
    "            - \"X\": A sequence of embeddings (List[List[float]] or tensor of shape \\\n",
    "                (seq_len, embed_dim))\n",
    "            - \"y\": A scalar target value (float)\n",
    "            - \"trip_id\": An identifier for the trip (int)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing:\n",
    "            - \"X\": Tensor of shape (batch_size, max_seq_len, embed_dim) with padded sequences\n",
    "            - \"y\": Tensor of shape (batch_size,) with target durations\n",
    "            - \"trip_id\": List of trip identifiers\n",
    "            - \"lengths\": Tensor of shape (batch_size,) with original sequence lengths\n",
    "    \"\"\"\n",
    "    X = [torch.tensor(item[\"X\"], dtype=torch.float32) for item in batch]\n",
    "    y = torch.tensor([item[\"y\"] for item in batch], dtype=torch.float32)\n",
    "    indexes = [item[\"trip_id\"] for item in batch]\n",
    "    lengths = [x.size(0) for x in X]  # original sequence lengths\n",
    "\n",
    "    X_padded = pad_sequence(X, batch_first=True)\n",
    "    return {\n",
    "        \"X\": X_padded,\n",
    "        \"y\": y,\n",
    "        \"trip_id\": indexes,\n",
    "        \"lengths\": torch.tensor(lengths, dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    dev_dataloader: DataLoader,\n",
    "    evaluator: object,\n",
    "    device: Union[str, torch.device] = \"cuda\",\n",
    "    epochs: int = 30,\n",
    "    lr: float = 1e-3,\n",
    "    save_dir: str = \"./\",\n",
    ") -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Trains a model with early stopping based on validation loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "        dev_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "        evaluator (object): An evaluator object with `_compute_metrics(preds, targets)` method.\n",
    "        device (Union[str, torch.device]): Device to train the model on ('cuda' or 'cpu').\n",
    "        epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        save_dir (str): Directory to save the best model weights.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, float]]: A list of dictionaries with evaluation metrics for each epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.L1Loss()\n",
    "\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    stop_counter = 0\n",
    "    prev_eval_loss = np.inf\n",
    "    loss_eval = []\n",
    "    loss_train = []\n",
    "    metrics_results = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss_list = []\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch: {epoch+1}\"):\n",
    "            inputs = batch[\"X\"].to(device)\n",
    "            lengths = batch[\"lengths\"].to(device)\n",
    "            labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "            outputs = model(inputs, lengths)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss_list.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(batch_loss_list)\n",
    "        loss_train.append(avg_train_loss)\n",
    "        logging.info(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        batch_eval_loss = []\n",
    "        metrics_per_batch = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(\n",
    "                tqdm(dev_dataloader, desc=\"Evaluation\", total=len(dev_dataloader))\n",
    "            ):\n",
    "                inputs = batch[\"X\"].to(device)\n",
    "                lengths = batch[\"lengths\"].to(device)\n",
    "                labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "                outputs = model(inputs, lengths)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                batch_eval_loss.append(loss.item())\n",
    "\n",
    "                metrics = evaluator._compute_metrics(outputs.cpu().numpy(), labels.cpu().numpy())\n",
    "                metrics_per_batch.append({\"Batch\": i, **metrics})\n",
    "\n",
    "        avg_eval_loss = np.mean(batch_eval_loss)\n",
    "        loss_eval.append(avg_eval_loss)\n",
    "        logging.info(f\"Evaluation Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "        mean_metrics = {\n",
    "            key: np.mean([b[key] for b in metrics_per_batch])\n",
    "            for key in metrics_per_batch[0].keys()\n",
    "            if key != \"Batch\"\n",
    "        }\n",
    "        metrics_results.append(mean_metrics)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_eval_loss >= prev_eval_loss:\n",
    "            stop_counter += 1\n",
    "            logging.info(f\"No improvement. Early stop counter: {stop_counter}/5\")\n",
    "            if stop_counter == 5:\n",
    "                logging.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(best_weights)\n",
    "                break\n",
    "        else:\n",
    "            stop_counter = 0\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        prev_eval_loss = avg_eval_loss\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"best_travel_time_model.pkl\"))\n",
    "    logging.info(\"Best model saved.\")\n",
    "    return model, metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = train_with_early_stopping(\n",
    "    model, train_dataloader, dev_dataloader, evaluator, device, 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "trip_indexes = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(\n",
    "        tqdm(test_dataloader, desc=\"Predicting...\", total=len(test_dataloader))\n",
    "    ):\n",
    "        inputs = batch[\"X\"].to(device)\n",
    "        lengths = batch[\"lengths\"].to(device)\n",
    "        indexes = batch[\"trip_id\"]\n",
    "        outputs = model(inputs, lengths)\n",
    "        trip_indexes.extend(indexes)\n",
    "        all_predictions.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(\n",
    "    dataset=porto_taxi, predictions=all_predictions, trip_ids=trip_indexes, log_metrics=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
