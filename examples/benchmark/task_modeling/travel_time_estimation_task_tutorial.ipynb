{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from collections.abc import Iterator\n",
    "from typing import Any, Callable, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from srai.benchmark import BaseEvaluator, TrajectoryRegressionEvaluator\n",
    "from srai.datasets import PortoTaxiDataset\n",
    "from srai.embedders import Hex2VecEmbedder  # noqa: F401\n",
    "from srai.h3 import ring_buffer_h3_regions_gdf\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders.osm_loaders import OSMPbfLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.regionalizers import H3Regionalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi = PortoTaxiDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = porto_taxi.load(version=\"TTE\")\n",
    "train, test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resolution = porto_taxi.resolution\n",
    "trip_ids_column = porto_taxi.target\n",
    "embedder_hidden_sizes = [150, 75, 25]\n",
    "regionalizer = H3Regionalizer(resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = test.sample(frac=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dev split from train split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = porto_taxi.train_test_split_bucket_trajectory(\n",
    "    trajectory_id_column=trip_ids_column,\n",
    "    task=\"TTE\",\n",
    "    test_size=0.1,\n",
    "    bucket_number=3,\n",
    "    dev=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.sample(frac=0.2, random_state=42)\n",
    "# dev = dev.sample(frac=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi.categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi.numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linestring embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = train.copy()\n",
    "dev_ = dev.copy()\n",
    "test_ = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train = regionalizer.transform(train_)\n",
    "regions_dev = regionalizer.transform(dev_)\n",
    "regions_test = regionalizer.transform(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the complete set of regions, ensuring coverage of any gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_geometry = regions_train.unary_union.union_all().buffer(0.1)\n",
    "\n",
    "full_regions = regionalizer.transform(\n",
    "    gpd.GeoDataFrame([\"full\"], geometry=[full_geometry]).set_crs(regions_train.crs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_regions_train = ring_buffer_h3_regions_gdf(regions_train, 2)\n",
    "buffered_regions_test = ring_buffer_h3_regions_gdf(regions_test, 2)\n",
    "buffered_regions_dev = ring_buffer_h3_regions_gdf(regions_dev, 2)\n",
    "\n",
    "osm_features = OSMPbfLoader().load(full_regions, HEX2VEC_FILTER)\n",
    "region_intersect_train = IntersectionJoiner().transform(buffered_regions_train, osm_features)\n",
    "\n",
    "# for H2V\n",
    "embedder = Hex2VecEmbedder(embedder_hidden_sizes)\n",
    "neighbourhood = H3Neighbourhood(buffered_regions_train)\n",
    "\n",
    "# # for CCE or CE\n",
    "# neighbourhood = H3Neighbourhood(full_regions)\n",
    "# embedder = ContextualCountEmbedder(neighbourhood=neighbourhood,\n",
    "#                                    neighbourhood_distance=2,\n",
    "#                                    expected_output_features=HEX2VEC_FILTER,\n",
    "#                                    concatenate_vectors=True,\n",
    "#                                    count_subcategories=True)\n",
    "# embedder = CountEmbedder(expected_output_features=HEX2VEC_FILTER)\n",
    "\n",
    "# # for GV\n",
    "# embedder = GeoVexEmbedder(target_features=HEX2VEC_FILTER, neighbourhood_radius=2)\n",
    "# neighbourhood = H3Neighbourhood(full_regions)\n",
    "\n",
    "# Neccessary for GV and H2V. For CCE and CE comment out this block\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    embedder.fit(\n",
    "        regions_gdf=buffered_regions_train,\n",
    "        features_gdf=osm_features,\n",
    "        joint_gdf=region_intersect_train,\n",
    "        neighbourhood=neighbourhood,\n",
    "        trainer_kwargs={\"max_epochs\": 10, \"accelerator\": device},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = embedder.transform(\n",
    "    regions_gdf=regions_train,\n",
    "    features_gdf=osm_features,\n",
    "    joint_gdf=region_intersect_train,\n",
    ")\n",
    "embeddings_train[\"h3\"] = embeddings_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_features_dev = OSMPbfLoader().load(buffered_regions_dev, HEX2VEC_FILTER)\n",
    "osm_features_test = OSMPbfLoader().load(buffered_regions_test, HEX2VEC_FILTER)\n",
    "\n",
    "region_intersect_dev = IntersectionJoiner().transform(regions_dev, osm_features_dev)\n",
    "region_intersect_test = IntersectionJoiner().transform(buffered_regions_test, osm_features_test)\n",
    "\n",
    "embeddings_dev = embedder.transform(\n",
    "    regions_gdf=buffered_regions_dev,\n",
    "    features_gdf=osm_features_dev,\n",
    "    joint_gdf=region_intersect_dev,\n",
    ")\n",
    "embeddings_test = embedder.transform(\n",
    "    regions_gdf=buffered_regions_test,\n",
    "    features_gdf=osm_features_test,\n",
    "    joint_gdf=region_intersect_test,\n",
    ")\n",
    "embeddings_test[\"h3\"] = embeddings_test.index\n",
    "embeddings_dev[\"h3\"] = embeddings_dev.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns(row: gpd.GeoSeries) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate embedding values together.\n",
    "\n",
    "    Args:\n",
    "        row (gpd.GeoSeries): row of embeddings\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: concatenated embedding\n",
    "    \"\"\"\n",
    "    return np.concatenate([np.atleast_1d(float(val)) for val in row.values]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = embeddings_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "embeddings_train[\"embedding\"] = embeddings_train[numeric_cols].apply(concat_columns, axis=1)\n",
    "embeddings_dev[\"embedding\"] = embeddings_dev[numeric_cols].apply(concat_columns, axis=1)\n",
    "embeddings_test[\"embedding\"] = embeddings_test[numeric_cols].apply(concat_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_embeddings_to_trips(traj_df, embedding_df, trip_ids_column: str = \"trip_id\"):\n",
    "    \"\"\"\n",
    "    Adds a column to traj_df with a list of embedding vectors matching the h3 sequence.\n",
    "\n",
    "    Args:\n",
    "        traj_df (pd.DataFrame): A DataFrame with columns [trip_ids_column (f.e \"trip_id\"),\n",
    "            \"duration\", \"h3_sequence\"].\n",
    "        embedding_df (pd.DataFrame): Index is h3 index, values are embedding vectors.\n",
    "        trip_ids_column (str): Column name for unique identifier of each trip.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: New DataFrame with an additional 'embedding_sequence' column.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding_sequence(h3_seq):\n",
    "        embeddings = []\n",
    "        for h in h3_seq:\n",
    "            if h in embedding_df.index:\n",
    "                emb = embedding_df.loc[h]\n",
    "                # If the embedding is a Series, convert to numpy array\n",
    "                if hasattr(emb, \"values\"):\n",
    "                    emb = emb.values\n",
    "                embeddings.append(emb)\n",
    "            else:\n",
    "                # Handle missing h3 (e.g., pad with zeros or skip)\n",
    "                embeddings.append(np.zeros(embedding_df.shape[1]))\n",
    "        return embeddings\n",
    "\n",
    "    traj_df = traj_df.copy()\n",
    "    traj_df[\"embedding_sequence\"] = traj_df[\"h3_sequence\"].apply(get_embedding_sequence)\n",
    "    return traj_df[[trip_ids_column, \"duration\", \"h3_sequence\", \"embedding_sequence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = attach_embeddings_to_trips(train, embeddings_train[\"embedding\"])\n",
    "merged_dev = attach_embeddings_to_trips(dev, embeddings_dev[\"embedding\"])\n",
    "merged_test = attach_embeddings_to_trips(test, embeddings_test[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(\n",
    "    df: pd.DataFrame, trip_ids_column: str = \"trip_id\"\n",
    ") -> Iterator[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generator function to yield training examples from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing columns:\n",
    "            - \"embedding_sequence\": A list of embedding vectors for each H3 hex in the trajectory\n",
    "              (i.e., List[List[float]] or numpy.ndarray of shape (seq_len, embed_dim)).\n",
    "            - \"trip_ids_column\": Column name for unique identifier of each trip.\n",
    "            - \"duration\": Target variable for the trip duration (e.g., a float or int).\n",
    "        trip_ids_column (str, optional): Name of the column containing trip IDs.\n",
    "            Defaults to \"trip_id\".\n",
    "\n",
    "\n",
    "    Yields:\n",
    "        Dict[str, Any]: A dictionary with keys:\n",
    "            - \"X\": The embedding sequence representing the trajectory.\n",
    "            - \"trip_id\": The unique trip identifier.\n",
    "            - \"y\": The target duration for the trip.\n",
    "    \"\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        yield {\n",
    "            \"X\": row[\"embedding_sequence\"],  # shape: (seq_len, embed_dim), as list\n",
    "            \"trip_id\": row[trip_ids_column],  # list of h3 indexes\n",
    "            \"y\": row[\"duration\"],  # target\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_generator(\n",
    "    lambda: generate_examples(merged_train, trip_ids_column=trip_ids_column)\n",
    ")\n",
    "dev_dataset = Dataset.from_generator(\n",
    "    lambda: generate_examples(merged_dev, trip_ids_column=trip_ids_column)\n",
    ")\n",
    "test_dataset = Dataset.from_generator(\n",
    "    lambda: generate_examples(merged_test, trip_ids_column=trip_ids_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trajectory model module.\n",
    "\n",
    "This module contains implementation of base model of trajectory.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TravelTimePredictionBaseModel(nn.Module):  # type: ignore\n",
    "    \"\"\"\n",
    "    Travel time prediction base model.\n",
    "\n",
    "    Definition of travel time prediction model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int):\n",
    "        \"\"\"\n",
    "        Initialization of travel time prediction module.\n",
    "\n",
    "        Args:\n",
    "            input_size: number of input features\n",
    "            hidden_size:  number of features in the hidden state of the LSTM\n",
    "            num_layers: The number of recurrent layers in the LSTM\n",
    "            output_size: number of output features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Padded input tensor of shape (batch_size, seq_len, input_size), \\\n",
    "                where `seq_len` is the maximum sequence length in the batch and `input_size` \\\n",
    "                is the dimensionality of each timestep's feature vector (e.g., embedding size).\n",
    "            lengths (torch.Tensor): 1D tensor of shape (batch_size,) containing the original \\\n",
    "                (unpadded) lengths of each sequence in the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_size), containing the \\\n",
    "                predicted values for each sequence in the batch.\n",
    "        \"\"\"\n",
    "        # Handling varying length of sequences\n",
    "        packed_input = pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # hn: (num_layers, batch_size, hidden_size)\n",
    "        # We can use the last layer's hidden state for regression\n",
    "        final_hidden = hn[-1]  # (batch_size, hidden_size)\n",
    "\n",
    "        out = self.fc(final_hidden)\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sample_input = merged_train[\"embedding_sequence\"].iloc[0]\n",
    "# len of single h3 embedding\n",
    "input_size = sample_input[0].shape[0]\n",
    "output_size = 1  # Predicting total duration (regression)\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "model = TravelTimePredictionBaseModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    ")\n",
    "evaluator = TrajectoryRegressionEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to handle variable-length sequences.\n",
    "\n",
    "    Pads a batch of embedding sequences to the maximum sequence length in the batch,\n",
    "    and prepares corresponding labels and metadata for model input.\n",
    "\n",
    "    Args:\n",
    "        batch (List[dict[str, Any]]): A list of examples, where each example is a dictionary\n",
    "            containing:\n",
    "            - \"X\": A sequence of embeddings (List[List[float]] or tensor of shape \\\n",
    "                (seq_len, embed_dim))\n",
    "            - \"y\": A scalar target value (float)\n",
    "            - \"trip_id\": An identifier for the trip (int)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing:\n",
    "            - \"X\": Tensor of shape (batch_size, max_seq_len, embed_dim) with padded sequences\n",
    "            - \"y\": Tensor of shape (batch_size,) with target durations\n",
    "            - \"trip_id\": List of trip identifiers\n",
    "            - \"lengths\": Tensor of shape (batch_size,) with original sequence lengths\n",
    "    \"\"\"\n",
    "    X = [torch.tensor(item[\"X\"], dtype=torch.float32) for item in batch]\n",
    "    y = torch.tensor([item[\"y\"] for item in batch], dtype=torch.float32)\n",
    "    indexes = [item[\"trip_id\"] for item in batch]\n",
    "    lengths = [x.size(0) for x in X]  # original sequence lengths\n",
    "\n",
    "    X_padded = pad_sequence(X, batch_first=True)\n",
    "    return {\n",
    "        \"X\": X_padded,\n",
    "        \"y\": y,\n",
    "        \"trip_id\": indexes,\n",
    "        \"lengths\": torch.tensor(lengths, dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    dev_dataloader: DataLoader,\n",
    "    evaluator: BaseEvaluator,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: Union[str, torch.device] = \"cuda\",\n",
    "    epochs: int = 30,\n",
    "    save_dir: str = \"./\",\n",
    "    early_stopping_patience: int = 5,\n",
    ") -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Trains a model with early stopping based on validation loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "        dev_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "        loss_fn (Callable): Loss function used for training (e.g., nn.MSELoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "        evaluator (Any): Object with a `_compute_metrics` method that accepts predicted and\n",
    "            target values (as numpy arrays) and returns a dictionary of metric results.\n",
    "        device (Union[str, torch.device]): Device to train the model on ('cuda' or 'cpu').\n",
    "        epochs (int): Number of training epochs.\n",
    "        save_dir (str): Directory to save the best model weights.\n",
    "        early_stopping_patience (int, optional): Number of evaluations without improvement\n",
    "            before early stopping. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, float]]: A list of dictionaries with evaluation metrics for each epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    stop_counter = 0\n",
    "    prev_eval_loss = np.inf\n",
    "    loss_eval = []\n",
    "    loss_train = []\n",
    "    metrics_results = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        print(f\"epoch:{epoch}\")\n",
    "        model.train()\n",
    "        batch_loss_list = []\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch: {epoch+1}\"):\n",
    "            inputs = batch[\"X\"].to(device)\n",
    "            lengths = batch[\"lengths\"].to(device)\n",
    "            labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "            outputs = model(inputs, lengths)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss_list.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(batch_loss_list)\n",
    "        loss_train.append(avg_train_loss)\n",
    "        logging.info(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        batch_eval_loss = []\n",
    "        metrics_per_batch = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(\n",
    "                tqdm(dev_dataloader, desc=\"Evaluation\", total=len(dev_dataloader))\n",
    "            ):\n",
    "                inputs = batch[\"X\"].to(device)\n",
    "                lengths = batch[\"lengths\"].to(device)\n",
    "                labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "                outputs = model(inputs, lengths)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                batch_eval_loss.append(loss.item())\n",
    "\n",
    "                metrics = evaluator._compute_metrics(outputs.cpu().numpy(), labels.cpu().numpy())\n",
    "                metrics_per_batch.append({\"Batch\": i, **metrics})\n",
    "\n",
    "        avg_eval_loss = np.mean(batch_eval_loss)\n",
    "        loss_eval.append(avg_eval_loss)\n",
    "        logging.info(f\"Evaluation Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "        mean_metrics = {\n",
    "            key: np.mean([b[key] for b in metrics_per_batch])\n",
    "            for key in metrics_per_batch[0].keys()\n",
    "            if key != \"Batch\"\n",
    "        }\n",
    "        metrics_results.append(mean_metrics)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_eval_loss >= prev_eval_loss:\n",
    "            stop_counter += 1\n",
    "            logging.info(f\"No improvement. Early stop counter: {stop_counter}/5\")\n",
    "            if stop_counter == early_stopping_patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(best_weights)\n",
    "                break\n",
    "        else:\n",
    "            stop_counter = 0\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        prev_eval_loss = avg_eval_loss\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"best_travel_time_model.pkl\"))\n",
    "    logging.info(\"Best model saved.\")\n",
    "    return model, metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.L1Loss()\n",
    "epochs = 70\n",
    "\n",
    "model, metrics = train_with_early_stopping(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    dev_dataloader=dev_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    "    early_stopping_patience=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "trip_indexes = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(\n",
    "        tqdm(test_dataloader, desc=\"Predicting...\", total=len(test_dataloader))\n",
    "    ):\n",
    "        inputs = batch[\"X\"].to(device)\n",
    "        lengths = batch[\"lengths\"].to(device)\n",
    "        indexes = batch[\"trip_id\"]\n",
    "        outputs = model(inputs, lengths)\n",
    "        trip_indexes.extend(indexes)\n",
    "        all_predictions.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(\n",
    "    dataset=porto_taxi,\n",
    "    predictions=all_predictions,\n",
    "    trip_ids=trip_indexes,\n",
    "    log_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
