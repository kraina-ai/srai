{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from typing import Any, Callable, Optional, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from examples.benchmark.task_modeling.coords import get_coords\n",
    "from geopy.distance import great_circle\n",
    "from srai.benchmark import BaseEvaluator, MobilityPredictionEvaluator\n",
    "from srai.datasets import PortoTaxiDataset\n",
    "from srai.embedders import Hex2VecEmbedder  # noqa: F401\n",
    "from srai.h3 import h3_to_geoseries\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders.osm_loaders import OSMPbfLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.regionalizers import H3Regionalizer, geocode_to_region_gdf\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi = PortoTaxiDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = porto_taxi.load(version=\"HMP\")\n",
    "train, test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedder_hidden_sizes = [150, 75, 25]\n",
    "resolution = porto_taxi.resolution\n",
    "regionalizer = H3Regionalizer(resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = test.sample(frac=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = porto_taxi.train_test_split(\n",
    "    trajectory_id_column=porto_taxi.target,\n",
    "    task=\"HMP\",\n",
    "    test_size=0.1,\n",
    "    n_bins=3,\n",
    "    validation_split=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(porto_taxi.dev_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.sample(frac=0.01, random_state=42)\n",
    "# dev = dev.sample(frac=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi.categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_taxi.numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linestring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = train.copy()\n",
    "dev_ = dev.copy()\n",
    "test_ = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train = regionalizer.transform(train_)\n",
    "regions_dev = regionalizer.transform(dev_)\n",
    "regions_test = regionalizer.transform(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = geocode_to_region_gdf(\"Porto, Portugal\")\n",
    "regions = regionalizer.transform(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_regions = regionalizer.transform(\n",
    "    gpd.GeoDataFrame(\n",
    "        [\"full\"], geometry=[regions_train.union_all().convex_hull]\n",
    "    ).set_crs(regions_train.crs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.h3 import ring_buffer_h3_regions_gdf\n",
    "\n",
    "buffered_regions_train = ring_buffer_h3_regions_gdf(regions_train, 2)\n",
    "buffered_regions_dev = ring_buffer_h3_regions_gdf(regions_dev, 2)\n",
    "buffered_regions_test = ring_buffer_h3_regions_gdf(regions_test, 2)\n",
    "\n",
    "\n",
    "osm_features = OSMPbfLoader().load(full_regions, HEX2VEC_FILTER)\n",
    "region_intersect_train = IntersectionJoiner().transform(\n",
    "    buffered_regions_train, osm_features\n",
    ")\n",
    "\n",
    "# # For CCE or CE usage\n",
    "# neighbourhood = H3Neighbourhood(full_regions)\n",
    "# embedder = ContextualCountEmbedder(neighbourhood=neighbourhood,\n",
    "#                                    neighbourhood_distance=2,\n",
    "#                                    expected_output_features=HEX2VEC_FILTER,\n",
    "#                                     concatenate_vectors=True,\n",
    "#                                     count_subcategories=True)\n",
    "# embedder = CountEmbedder(expected_output_features=HEX2VEC_FILTER)\n",
    "\n",
    "# # For H2V usage\n",
    "embedder = Hex2VecEmbedder(embedder_hidden_sizes)\n",
    "neighbourhood = H3Neighbourhood(buffered_regions_train)\n",
    "\n",
    "\n",
    "# # For GV usage\n",
    "# embedder = GeoVexEmbedder(target_features=HEX2VEC_FILTER, neighbourhood_radius=2)\n",
    "# neighbourhood = H3Neighbourhood(full_regions)\n",
    "\n",
    "# Needed for H2V and GV. Comment fitting block out for CCE and CE\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    embedder.fit(\n",
    "        regions_gdf=buffered_regions_train,\n",
    "        features_gdf=osm_features,\n",
    "        joint_gdf=region_intersect_train,\n",
    "        neighbourhood=neighbourhood,\n",
    "        trainer_kwargs={\"max_epochs\": 10, \"accelerator\": device},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_intersect = IntersectionJoiner().transform(full_regions, osm_features)\n",
    "all_embeddings = embedder.transform(\n",
    "    regions_gdf=full_regions, features_gdf=osm_features, joint_gdf=region_intersect\n",
    ")\n",
    "all_embeddings[\"h3\"] = all_embeddings.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = embedder.transform(\n",
    "    regions_gdf=buffered_regions_train,\n",
    "    features_gdf=osm_features,\n",
    "    joint_gdf=region_intersect_train,\n",
    ")\n",
    "embeddings_train[\"h3\"] = embeddings_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_intersect_dev = IntersectionJoiner().transform(\n",
    "    buffered_regions_dev, osm_features\n",
    ")\n",
    "region_intersect_test = IntersectionJoiner().transform(\n",
    "    buffered_regions_test, osm_features\n",
    ")\n",
    "\n",
    "embeddings_dev = embedder.transform(\n",
    "    regions_gdf=buffered_regions_dev,\n",
    "    features_gdf=osm_features,\n",
    "    joint_gdf=region_intersect_dev,\n",
    ")\n",
    "embeddings_dev[\"h3\"] = embeddings_dev.index\n",
    "\n",
    "embeddings_test = embedder.transform(\n",
    "    regions_gdf=buffered_regions_test,\n",
    "    features_gdf=osm_features,\n",
    "    joint_gdf=region_intersect_test,\n",
    ")\n",
    "embeddings_test[\"h3\"] = embeddings_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns(row: gpd.GeoSeries) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate embedding values together.\n",
    "\n",
    "    Args:\n",
    "        row (gpd.GeoSeries): row of embeddings\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: concatenated embedding\n",
    "    \"\"\"\n",
    "    return np.concatenate([np.atleast_1d(float(val)) for val in row.values]).astype(\n",
    "        np.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = all_embeddings.select_dtypes(include=[np.number]).columns\n",
    "all_embeddings[\"embedding\"] = all_embeddings[numeric_cols].apply(concat_columns, axis=1)\n",
    "embeddings_train[\"embedding\"] = embeddings_train[numeric_cols].apply(\n",
    "    concat_columns, axis=1\n",
    ")\n",
    "embeddings_dev[\"embedding\"] = embeddings_dev[numeric_cols].apply(concat_columns, axis=1)\n",
    "embeddings_test[\"embedding\"] = embeddings_test[numeric_cols].apply(\n",
    "    concat_columns, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_embeddings_to_trips(\n",
    "    traj_df: pd.DataFrame, embedding_df: pd.DataFrame, trip_ids_column: str = \"trip_id\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach embedding sequences to H3-indexed trajectories.\n",
    "\n",
    "    For each row in the trajectory DataFrame, this function looks up the corresponding\n",
    "    H3 indices in the `embedding_df` and constructs sequences of embedding vectors\n",
    "    (as NumPy arrays) for both `h3_sequence_x`.\n",
    "\n",
    "    If an H3 index is missing in the `embedding_df`, it is replaced with a zero vector\n",
    "    of the appropriate embedding dimension.\n",
    "\n",
    "    Args:\n",
    "        traj_df (pd.DataFrame): A DataFrame with columns:\n",
    "            - 'trip_columns_id': unique trip identifiers.\n",
    "            - \"h3_sequence_x\": list of H3 indices (str) for the x-sequence.\n",
    "            - \"h3_sequence_y\": list of H3 indices (str) for the y-sequence.\n",
    "        embedding_df (pd.DataFrame): A DataFrame where the index consists of H3 indices\n",
    "            and each row contains a corresponding embedding vector.\n",
    "        trip_ids_column (str): Column name for unique identifier of each trip.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: pd.DataFrame: New DataFrame with an additional 'embedding_sequence_x' column.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_embedding_sequence(h3_seq: list[str]) -> list[np.ndarray]:\n",
    "        embeddings: list[np.ndarray] = []\n",
    "        for h in h3_seq:\n",
    "            if h in embedding_df.index:\n",
    "                emb: Any = embedding_df.loc[h]\n",
    "                if hasattr(emb, \"values\"):\n",
    "                    emb = emb.values\n",
    "                embeddings.append(emb)\n",
    "            else:\n",
    "                embeddings.append(np.zeros(len(embedding_df.iloc[0])))\n",
    "        return embeddings\n",
    "\n",
    "    traj_df = traj_df.copy()\n",
    "    traj_df[\"embedding_sequence_x\"] = traj_df[\"h3_sequence_x\"].apply(\n",
    "        get_embedding_sequence\n",
    "    )\n",
    "\n",
    "    return traj_df[\n",
    "        [trip_ids_column, \"h3_sequence_x\", \"embedding_sequence_x\", \"h3_sequence_y\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = attach_embeddings_to_trips(train, embeddings_train[\"embedding\"])\n",
    "merged_dev = attach_embeddings_to_trips(dev, embeddings_dev[\"embedding\"])\n",
    "merged_test = attach_embeddings_to_trips(test, embeddings_test[\"embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper dataframes -> h3 to embeddings mapping and regions to neighbourhoods with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_embedding_lookup = all_embeddings[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_embedding_lookup_dict = dict(\n",
    "    zip(h3_embedding_lookup.index, h3_embedding_lookup.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_neighbours(h3_idx: str, h3_neighbourhood_size: int) -> list[int]:\n",
    "    return h3.grid_disk(h3_idx, k=h3_neighbourhood_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_regions[\"neighbours\"] = full_regions.index.map(\n",
    "    lambda x: _get_neighbours(x, h3_neighbourhood_size=1)\n",
    ")\n",
    "full_regions[\"labels\"] = full_regions[\"neighbours\"].apply(\n",
    "    lambda x: get_coords(x, x[0], 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lookup_df = full_regions.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that maps y sequence to a sequence of spatially consistent class labels (look at coords.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_h3_sequence(\n",
    "    h3_seq_x: list[str],\n",
    "    h3_seq_y: list[str],\n",
    "    label_lookup_df: pd.DataFrame,\n",
    ") -> list[Union[int, float]]:\n",
    "    \"\"\"\n",
    "    Generate a sequence of labels based on H3 transitions and a lookup table.\n",
    "\n",
    "    For each step in the H3 y-sequence, this function checks if the target H3 index\n",
    "    exists among the neighbors of the current H3 cell (starting from the last in `h3_seq_x`),\n",
    "    and assigns the corresponding label from the `label_lookup_df`.\n",
    "\n",
    "    If a target H3 index is not a neighbor or if an error occurs, a default label of 0 is used.\n",
    "    0 represents staying in the same hexagon.\n",
    "\n",
    "    Args:\n",
    "        h3_seq_x (List[str]): List of H3 indices representing the observed part of a trip.\n",
    "        h3_seq_y (List[str]): List of H3 indices representing the predicted/future path.\n",
    "        label_lookup_df (pd.DataFrame): DataFrame indexed by H3 index, with two columns:\n",
    "            - \"neighbours\": List[str] of neighbor H3 indices.\n",
    "            - \"labels\": List[int or float] of corresponding labels for transitions.\n",
    "\n",
    "    Returns:\n",
    "        List[Union[int, float]]: Sequence of labels aligned with `h3_seq_y`.\n",
    "    \"\"\"\n",
    "    labels: list[Union[int, float]] = []\n",
    "    current: str = h3_seq_x[-1]  # Start from the last known H3 in x\n",
    "\n",
    "    for next_h3 in h3_seq_y:\n",
    "        try:\n",
    "            label_row = label_lookup_df.loc[current]\n",
    "            if next_h3 in label_row[\"neighbours\"]:\n",
    "                idx = label_row[\"neighbours\"].index(next_h3)\n",
    "                label = label_row[\"labels\"][idx]\n",
    "            else:\n",
    "                label = 0\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            label = 0\n",
    "        labels.append(label)\n",
    "        current = next_h3\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train[\"h3_sequence_y_labels\"] = merged_train.apply(\n",
    "    lambda row: label_h3_sequence(\n",
    "        row[\"h3_sequence_x\"], row[\"h3_sequence_y\"], label_lookup_df\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "merged_dev[\"h3_sequence_y_labels\"] = merged_dev.apply(\n",
    "    lambda row: label_h3_sequence(\n",
    "        row[\"h3_sequence_x\"], row[\"h3_sequence_y\"], label_lookup_df\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "merged_test[\"h3_sequence_y_labels\"] = merged_test.apply(\n",
    "    lambda row: label_h3_sequence(\n",
    "        row[\"h3_sequence_x\"], row[\"h3_sequence_y\"], label_lookup_df\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3SequenceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading sequences of H3 indices and their embeddings.\n",
    "\n",
    "    Each item in the dataset is expected to include:\n",
    "        - \"h3_sequence_x\": input H3 index sequence\n",
    "        - \"h3_sequence_y\": target/predicted H3 index sequence\n",
    "        - \"embedding_sequence_x\": list of embedding vectors corresponding to h3_sequence_x\n",
    "        - \"h3_sequence_y_labels\": label sequence aligned with h3_sequence_y\n",
    "        - 'trip_ids_column': identifier of the trip\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing the required columns for the dataset.\n",
    "\n",
    "    Attributes:\n",
    "        df (pd.DataFrame): Internal reference to the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, trip_ids_column: str = \"trip_id\") -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset with a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to wrap.\n",
    "            trip_ids_column (str): Column name for unique identifier of each trip.\n",
    "        \"\"\"\n",
    "        self.trip_id_column: str = trip_ids_column\n",
    "        self.df: pd.DataFrame = df\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Retrieve one item from the dataset by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing sequence data and labels.\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            \"h3_sequence_x\": row[\"h3_sequence_x\"],\n",
    "            \"h3_sequence_y\": row[\"h3_sequence_y\"],\n",
    "            \"embedding_sequence_x\": row[\"embedding_sequence_x\"],\n",
    "            \"h3_sequence_y_labels\": row[\"h3_sequence_y_labels\"],\n",
    "            \"trip_id\": row[self.trip_id_column],\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the total number of items in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Collate function for PyTorch DataLoader when using batch size of 1.\n",
    "\n",
    "    This function converts the \"embedding_sequence_x\" and \"h3_sequence_y_labels\"\n",
    "    fields into torch tensors with appropriate data types.\n",
    "\n",
    "    Args:\n",
    "        batch (List[Dict[str, Any]]): A batch of samples, expected to contain one item.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A single item with tensor-converted fields.\n",
    "    \"\"\"\n",
    "    item = batch[0]  # Assumes batch_size == 1\n",
    "    item[\"embedding_sequence_x\"] = torch.tensor(\n",
    "        item[\"embedding_sequence_x\"], dtype=torch.float32\n",
    "    )\n",
    "    item[\"h3_sequence_y_labels\"] = torch.tensor(\n",
    "        item[\"h3_sequence_y_labels\"], dtype=torch.long\n",
    "    )\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_batch_size = 1\n",
    "train_dataset = H3SequenceDataset(merged_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=dataloader_batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "dev_dataset = H3SequenceDataset(merged_dev)\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "    dev_dataset, batch_size=dataloader_batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_dataset = H3SequenceDataset(merged_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=dataloader_batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition & helper functions related to training and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_h3(\n",
    "    current_h3: Union[str, int],\n",
    "    predicted_class: Any,\n",
    "    label_lookup_df: pd.DataFrame,\n",
    ") -> Union[str, int]:\n",
    "    \"\"\"\n",
    "    Retrieve the predicted H3 index based on the current H3 index and the predicted class label.\n",
    "\n",
    "    If the current H3 index is not in the lookup table, it computes its neighbors and their\n",
    "    corresponding labels on the fly.\n",
    "\n",
    "    Args:\n",
    "        current_h3 (Union[str, int]): The current H3 index (string or integer).\n",
    "        predicted_class (Any): The predicted class label to match with the label list.\n",
    "        label_lookup_df (pd.DataFrame): DataFrame with H3 indices as index, containing 'neighbours'\n",
    "            and 'labels' columns.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, int]: The predicted H3 index if found in neighbors, otherwise the current\\\n",
    "              H3 index.\n",
    "    \"\"\"\n",
    "    if current_h3 in label_lookup_df.index:\n",
    "        row = label_lookup_df.loc[current_h3]\n",
    "    else:\n",
    "        df = pd.DataFrame([current_h3], columns=[\"region_id\"])\n",
    "        df[\"neighbours\"] = df[\"region_id\"].map(\n",
    "            lambda x: _get_neighbours(x, h3_neighbourhood_size=1)\n",
    "        )\n",
    "        df[\"labels\"] = df[\"neighbours\"].apply(lambda x: get_coords(x, x[0], 1))\n",
    "        df.index = df[\"region_id\"]\n",
    "        row = df.loc[current_h3]\n",
    "\n",
    "    if predicted_class in row[\"labels\"]:\n",
    "        idx = row[\"labels\"].tolist().index(predicted_class)\n",
    "        return row[\"neighbours\"][idx]\n",
    "    else:\n",
    "        return current_h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to handle case of an h3 which embedding not present in the lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_h3(h3_index: Union[str, int]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute a feature embedding for a given H3 index using OSM-derived features.\n",
    "\n",
    "    This function transforms the H3 index into a geometry, loads corresponding\n",
    "    OSM features, intersects them with the region, and generates a numerical embedding.\n",
    "\n",
    "    Args:\n",
    "        h3_index (Union[str, int]): The H3 index for which to compute the embedding.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Pandas Series where the index is the H3 index and the value is a\n",
    "                   NumPy array representing the embedding vector.\n",
    "    \"\"\"\n",
    "    geo_series = h3_to_geoseries(h3_index)\n",
    "\n",
    "    # Ensure the CRS is correctly transformed\n",
    "    # if geo_series.crs != \"EPSG:4326\":\n",
    "    #     geo_series = geo_series.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    geometry = geo_series.geometry.iloc[0]\n",
    "\n",
    "    regions = gpd.GeoDataFrame(\n",
    "        geometry=[geometry],\n",
    "        index=pd.Index([h3_index], name=\"region_id\"),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    osm_features = OSMPbfLoader().load(regions, HEX2VEC_FILTER)\n",
    "    region_intersect = IntersectionJoiner().transform(regions, osm_features)\n",
    "\n",
    "    single_embedding = embedder.transform(\n",
    "        regions_gdf=regions, features_gdf=osm_features, joint_gdf=region_intersect\n",
    "    )\n",
    "\n",
    "    single_embedding[\"h3\"] = single_embedding.index\n",
    "    numeric_cols = single_embedding.select_dtypes(include=[np.number]).columns\n",
    "    single_embedding[\"embedding\"] = single_embedding[numeric_cols].apply(\n",
    "        concat_columns, axis=1\n",
    "    )\n",
    "\n",
    "    return single_embedding[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvaersine distance used ass additional loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two geographic coordinates using geopy.\n",
    "\n",
    "    Args:\n",
    "        lat1 (float): Latitude of the first point in decimal degrees.\n",
    "        lon1 (float): Longitude of the first point in decimal degrees.\n",
    "        lat2 (float): Latitude of the second point in decimal degrees.\n",
    "        lon2 (float): Longitude of the second point in decimal degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Distance between the two points in kilometers.\n",
    "    \"\"\"\n",
    "    return great_circle((lat1, lon1), (lat2, lon2)).kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader: DataLoader,\n",
    "    evaluator: BaseEvaluator,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    geo_loss_weight: float,\n",
    "    embedding_lookup: dict[str, Any],\n",
    "    label_lookup_df: pd.DataFrame,\n",
    "    epochs: int = 10,\n",
    "    save_dir: str = \"./\",\n",
    "    early_stopping_patience: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Trains a sequence prediction model on H3 embedding data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        dev_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        evaluator (BaseEvaluator): An BaseEvaluator with `_compute_metrics(preds, targets)` method.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "        loss_fn (Callable): Loss function (e.g., nn.CrossEntropyLoss).\n",
    "        geo_loss_weight (float): Weight of haversine distance added to the loss function.\n",
    "        embedding_lookup (Dict[str, Any]): Dictionary mapping H3 cell indexes to embedding vectors.\n",
    "        label_lookup_df (pd.DataFrame): DataFrame used to resolve neighboring H3 cells and labels.\n",
    "        epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "        save_dir (str): Directory to save the best model weights.\n",
    "        early_stopping_patience (int, optional): Number of evaluations without improvement\n",
    "            before early stopping. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    mean_metrics = []\n",
    "    best_model_state = None\n",
    "    best_eval_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_bar = tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch + 1}/{epochs} [Training]\", leave=False\n",
    "        )\n",
    "\n",
    "        for item in train_bar:\n",
    "            h3_seq_x = item[\"h3_sequence_x\"]\n",
    "            h3_seq_y = item[\"h3_sequence_y\"]\n",
    "            h3_embed_seq_x = item[\"embedding_sequence_x\"]\n",
    "            label_seq = item[\"h3_sequence_y_labels\"]\n",
    "\n",
    "            hidden = None\n",
    "            h3_embed_seq = h3_embed_seq_x.unsqueeze(0).to(\n",
    "                device\n",
    "            )  # [1, seq_len, embed_dim]\n",
    "            current_h3 = h3_seq_x[-1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0.0\n",
    "\n",
    "            for t in range(len(label_seq)):\n",
    "                logits, _ = model(h3_embed_seq, hidden)\n",
    "                target = torch.tensor([label_seq[t]], dtype=torch.long, device=device)\n",
    "\n",
    "                pred_class = logits.argmax(dim=1).item() + 1\n",
    "                true_h3 = h3_seq_y[t]\n",
    "                pred_h3 = get_predicted_h3(current_h3, pred_class, label_lookup_df)\n",
    "\n",
    "                # Get coordinates\n",
    "                true_lat, true_lon = h3.cell_to_latlng(true_h3)\n",
    "                pred_lat, pred_lon = h3.cell_to_latlng(pred_h3)\n",
    "                haversine_m = haversine_distance(true_lat, true_lon, pred_lat, pred_lon)\n",
    "                geo_loss = torch.tensor(haversine_m, dtype=torch.float, device=device)\n",
    "                log_geo_loss = torch.log1p(geo_loss)\n",
    "                loss += (1 - geo_loss_weight) * loss_fn(\n",
    "                    logits, target\n",
    "                ) + geo_loss_weight * log_geo_loss\n",
    "\n",
    "                # Teacher forcing: use actual next H3 from sequence\n",
    "                pred_h3 = h3_seq_y[t]\n",
    "                # if pred_h3 in embedding_lookup:\n",
    "                next_embedding = embedding_lookup[pred_h3]\n",
    "                # else:\n",
    "                #     next_embedding = embed_h3(pred_h3).get(pred_h3)\n",
    "\n",
    "                next_embedding_tensor = (\n",
    "                    torch.tensor(next_embedding, dtype=torch.float, device=device)\n",
    "                    .unsqueeze(0)\n",
    "                    .unsqueeze(0)\n",
    "                )  # [1, 1, embed_dim]\n",
    "\n",
    "                h3_embed_seq = torch.cat([h3_embed_seq, next_embedding_tensor], dim=1)\n",
    "                current_h3 = pred_h3\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() / len(label_seq)\n",
    "            train_bar.set_postfix(loss=loss.item() / len(label_seq))\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # ---- Evaluation ----\n",
    "        model.eval()\n",
    "        total_eval_loss = 0.0\n",
    "        batch_metrics = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            eval_bar = tqdm(dev_loader, desc=f\"Epoch {epoch + 1} [Eval]\", leave=False)\n",
    "\n",
    "            for item in eval_bar:\n",
    "                h3_seq_x = item[\"h3_sequence_x\"]\n",
    "                h3_seq_y = item[\"h3_sequence_y\"]\n",
    "                h3_embed_seq_x = item[\"embedding_sequence_x\"]\n",
    "                label_seq = item[\"h3_sequence_y_labels\"]\n",
    "\n",
    "                hidden = None\n",
    "                h3_embed_seq = h3_embed_seq_x.unsqueeze(0).to(device)\n",
    "                current_h3 = h3_seq_x[-1]\n",
    "                loss = 0.0\n",
    "\n",
    "                pred_h3_sequence: list[str] = []\n",
    "                true_h3_sequence: list[str] = h3_seq_y\n",
    "\n",
    "                for t in range(len(label_seq)):\n",
    "                    logits, _ = model(h3_embed_seq, hidden)\n",
    "                    target = torch.tensor(\n",
    "                        [label_seq[t]], dtype=torch.long, device=device\n",
    "                    )\n",
    "\n",
    "                    pred_class = logits.argmax(dim=1).item() + 1\n",
    "                    pred_h3 = get_predicted_h3(current_h3, pred_class, label_lookup_df)\n",
    "                    pred_h3_sequence.append(pred_h3)\n",
    "\n",
    "                    true_h3 = h3_seq_y[t]\n",
    "                    # Get coordinates\n",
    "                    true_lat, true_lon = h3.cell_to_latlng(true_h3)\n",
    "                    pred_lat, pred_lon = h3.cell_to_latlng(pred_h3)\n",
    "                    haversine_m = haversine_distance(\n",
    "                        true_lat, true_lon, pred_lat, pred_lon\n",
    "                    )\n",
    "                    geo_loss = torch.tensor(\n",
    "                        haversine_m, dtype=torch.float, device=device\n",
    "                    )\n",
    "                    log_geo_loss = torch.log1p(geo_loss)\n",
    "                    loss += (1 - geo_loss_weight) * loss_fn(\n",
    "                        logits, target\n",
    "                    ) + geo_loss_weight * log_geo_loss\n",
    "\n",
    "                    # if pred_h3 in embedding_lookup:\n",
    "                    next_embedding = embedding_lookup[pred_h3]\n",
    "                    # else:\n",
    "                    #     next_embedding = embed_h3(pred_h3).get(pred_h3)\n",
    "\n",
    "                    next_embedding_tensor = (\n",
    "                        torch.tensor(next_embedding, dtype=torch.float, device=device)\n",
    "                        .unsqueeze(0)\n",
    "                        .unsqueeze(0)\n",
    "                    )\n",
    "\n",
    "                    h3_embed_seq = torch.cat(\n",
    "                        [h3_embed_seq, next_embedding_tensor], dim=1\n",
    "                    )\n",
    "                    current_h3 = pred_h3\n",
    "\n",
    "                metrics = evaluator._compute_metrics(\n",
    "                    true_sequences=[true_h3_sequence], pred_sequences=[pred_h3_sequence]\n",
    "                )\n",
    "                batch_metrics.append(metrics)\n",
    "                total_eval_loss += loss.item() / len(label_seq)\n",
    "                eval_bar.set_postfix(loss=loss.item() / len(label_seq))\n",
    "\n",
    "        avg_eval_loss = total_eval_loss / len(dev_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Dev Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "        if avg_eval_loss < best_eval_loss:\n",
    "            best_eval_loss = avg_eval_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            stop_counter = 0\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "            print(f\"No improvement. Early stopping counter: {stop_counter}/5\")\n",
    "            if stop_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "    mean_metrics = {\n",
    "        metric: float(np.mean([m[metric] for m in batch_metrics]))\n",
    "        for metric in batch_metrics[0]\n",
    "    }\n",
    "    model.load_state_dict(best_model_state)\n",
    "    torch.save(\n",
    "        model.state_dict(), os.path.join(save_dir, \"best_mobility_prediction_model.pkl\")\n",
    "    )\n",
    "    logging.info(\"Best model saved.\")\n",
    "    return model, mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveH3Predictor(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequence model combining LSTM and Multi-Head Self-Attention for H3 index prediction.\n",
    "\n",
    "    This model processes a sequence of H3 embeddings using an LSTM to capture temporal dynamics,\n",
    "    followed by a self-attention mechanism to better aggregate contextual information across the\\\n",
    "          sequence.\n",
    "    It predicts the next H3 class label based on the final attended representation.\n",
    "\n",
    "    Args:\n",
    "        embedding_dim (int): Dimensionality of the input H3 embeddings. Default is 64.\n",
    "        hidden_dim (int): Dimensionality of the LSTM hidden states and attention. Default is 128.\n",
    "        num_classes (int): Number of output classes (e.g., neighboring H3 cells). Default is 7.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embedding_dim: int = 64, hidden_dim: int = 128, num_classes: int = 7\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize model with embedding dimensions, hidden dimensions and num classes.\"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tensor, hidden: Optional[tuple[Tensor, Tensor]] = None\n",
    "    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass of the AttentiveH3Predictor model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "            hidden (Optional[Tuple[Tensor, Tensor]]): Initial hidden and cell states for the LSTM.\n",
    "                Each is of shape (1, batch_size, hidden_dim), or None to initialize zero states.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "                - logits: Tensor of shape (batch_size, num_classes) representing class logits at \\\n",
    "                    the final timestep.\n",
    "                - hidden: Final LSTM hidden and cell states.\n",
    "        \"\"\"\n",
    "        out, hidden = self.lstm(x, hidden)  # out: (B, T, H)\n",
    "        attn_out, _ = self.attn(out, out, out)  # self-attention: (B, T, H)\n",
    "        logits = self.classifier(attn_out[:, -1, :])\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = h3_embedding_lookup.iloc[0].shape[0]\n",
    "model = AttentiveH3Predictor(embedding_dim=embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "geo_loss_weight = 0.7\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "evaluator = MobilityPredictionEvaluator()\n",
    "\n",
    "# Train\n",
    "model, metrics = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    evaluator,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    geo_loss_weight,\n",
    "    h3_embedding_lookup_dict,\n",
    "    label_lookup_df,\n",
    "    epochs=1,\n",
    "    early_stopping_patience=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_true_seqs = []\n",
    "all_pred_seqs = []\n",
    "trip_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting...\", total=len(test_loader)):\n",
    "        h3_seq_x = batch[\"h3_sequence_x\"]\n",
    "        h3_seq_y = batch[\"h3_sequence_y\"]\n",
    "        embed_seq_x = batch[\"embedding_sequence_x\"]\n",
    "        label_seq = batch[\"h3_sequence_y_labels\"]\n",
    "        trip_id = batch.get(\"trip_id\")\n",
    "\n",
    "        hidden = None\n",
    "        h3_embed_seq = embed_seq_x.unsqueeze(0).to(device)\n",
    "        current_h3 = h3_seq_x[-1]\n",
    "\n",
    "        pred_h3_sequence = []\n",
    "\n",
    "        for _ in range(len(label_seq)):\n",
    "            logits, _ = model(h3_embed_seq, hidden)\n",
    "\n",
    "            pred_class = logits.argmax(dim=1).item() + 1\n",
    "            pred_h3 = get_predicted_h3(current_h3, pred_class, label_lookup_df)\n",
    "            pred_h3_sequence.append(pred_h3)\n",
    "            if pred_h3 in h3_embedding_lookup_dict.keys():\n",
    "                next_embedding = h3_embedding_lookup_dict.get(pred_h3)\n",
    "            else:\n",
    "                next_embedding = embed_h3(pred_h3).get(pred_h3)\n",
    "            next_embedding_tensor = (\n",
    "                torch.tensor(next_embedding, dtype=torch.float, device=device)\n",
    "                .unsqueeze(0)\n",
    "                .unsqueeze(0)\n",
    "            )\n",
    "\n",
    "            h3_embed_seq = torch.cat([h3_embed_seq, next_embedding_tensor], dim=1)\n",
    "            current_h3 = pred_h3\n",
    "\n",
    "        all_true_seqs.append(h3_seq_y)\n",
    "        all_pred_seqs.append(pred_h3_sequence)\n",
    "\n",
    "        if trip_id is not None:\n",
    "            trip_ids.append(trip_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate for different sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 3, 5, 7, 10]:\n",
    "    evaluator = MobilityPredictionEvaluator(k=k)\n",
    "    print(f\"Evaluating with k={k}...\")\n",
    "    metrics = evaluator.evaluate(\n",
    "        porto_taxi, predictions=all_pred_seqs, trip_ids=trip_ids, log_metrics=False\n",
    "    )\n",
    "\n",
    "    print(f\"Metrics for k={k}: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize predictions vs true data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_seqs = [s for s in all_true_seqs if len(s) > 4]\n",
    "p_seqs = [s for s in all_pred_seqs if len(s) > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 0\n",
    "\n",
    "s_true = gpd.GeoDataFrame(\n",
    "    t_seqs[ID], geometry=h3_to_geoseries(t_seqs[ID]), columns=[\"h3\"]\n",
    ")\n",
    "s_pred = gpd.GeoDataFrame(\n",
    "    p_seqs[ID], geometry=h3_to_geoseries(p_seqs[ID]), columns=[\"h3\"]\n",
    ")\n",
    "\n",
    "print(len(s_true), len(s_pred))\n",
    "\n",
    "m = s_true.explore()\n",
    "s_pred.explore(m=m, color=\"red\", name=\"Predicted H3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
