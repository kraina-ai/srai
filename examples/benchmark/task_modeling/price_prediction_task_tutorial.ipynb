{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from srai.benchmark import BaseEvaluator, HexRegressionEvaluator\n",
    "from srai.datasets import AirbnbMulticityDataset\n",
    "from srai.embedders import Hex2VecEmbedder  # noqa: F401\n",
    "from srai.h3 import h3_to_geoseries, ring_buffer_h3_regions_gdf\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders.osm_loaders import OSMPbfLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.plotting import plot_numeric_data\n",
    "from srai.regionalizers import H3Regionalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resolution = 9\n",
    "embedder_hidden_sizes = [150, 75, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "regionalizer = H3Regionalizer(resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = AirbnbMulticityDataset()\n",
    "ds = airbnb.load(version=str(resolution))\n",
    "train, test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dev split from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = airbnb.train_test_split_bucket_regression(test_size=0.1, dev=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about available categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb.categorical_columns, airbnb.numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the complete set of regions, ensuring coverage of any gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = train.copy()\n",
    "dev_ = dev.copy()\n",
    "test_ = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for house sales in king county\n",
    "# regions_train = regionalizer.transform(train_)\n",
    "# full_geometry = regions_train.unary_union.union_all().buffer(0.1)\n",
    "\n",
    "# full_regions = regionalizer.transform(\n",
    "#     gpd.GeoDataFrame([\"full\"], geometry=[full_geometry]).set_crs(regions_train.crs)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for airbnb dataset. As there is multiple cities across the world, using previous\n",
    "# previous sell will take up a lot of RAM further in the process\n",
    "full_regions_list = []\n",
    "\n",
    "for city, group in train_.groupby(\"city\"):\n",
    "    city_ = group.copy()\n",
    "\n",
    "    # Apply regionalization to the city subset\n",
    "    regions_city = regionalizer.transform(city_)\n",
    "    # Compute convex hull + buffer over all regions\n",
    "    full_geometry = regions_city.union_all().buffer(0.1)\n",
    "\n",
    "    # Convert full_geometry to a GeoDataFrame\n",
    "    full_region_gdf = gpd.GeoDataFrame(\n",
    "        {\"city\": [city]}, geometry=[full_geometry], crs=regions_city.crs\n",
    "    )\n",
    "\n",
    "    # Apply regionalizer again on the full_geometry\n",
    "    city_regions = regionalizer.transform(full_region_gdf)\n",
    "\n",
    "    # Optionally add city name to the result for traceability\n",
    "    city_regions[\"city\"] = city\n",
    "\n",
    "    # Collect the result\n",
    "    full_regions_list.append(city_regions)\n",
    "\n",
    "# Concatenate all full_regions into a single GeoDataFrame\n",
    "full_regions = gpd.GeoDataFrame(pd.concat(full_regions_list, ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get h3 indexes for data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train = regionalizer.transform(train_)\n",
    "joined_train = gpd.sjoin(train_, regions_train, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_train.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)\n",
    "\n",
    "regions_dev = regionalizer.transform(dev_)\n",
    "joined_dev = gpd.sjoin(dev_, regions_dev, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_dev.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)\n",
    "\n",
    "\n",
    "regions_test = regionalizer.transform(test_)\n",
    "joined_test = gpd.sjoin(test_, regions_test, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_test.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in case of using numerical columns\n",
    "# columns_to_add = airbnb.numerical_columns + [airbnb.target]\n",
    "# joined_train[airbnb.numerical_columns] = scaler.fit_transform(\n",
    "#     joined_train[airbnb.numerical_columns]\n",
    "# )\n",
    "# train_averages_hex = joined_train.groupby(\"h3_index\")[columns_to_add].mean()\n",
    "\n",
    "# joined_dev[airbnb.numerical_columns] = scaler.transform(joined_dev[airbnb.numerical_columns])\n",
    "# joined_test[airbnb.numerical_columns] = scaler.transform(joined_test[airbnb.numerical_columns])\n",
    "\n",
    "# dev_averages_hex = joined_dev.groupby(\"h3_index\")[columns_to_add].mean()\n",
    "# test_averages_hex = joined_test.groupby(\"h3_index\")[columns_to_add].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of only geospatial embeddings\n",
    "columns_to_add = [airbnb.target]\n",
    "train_averages_hex = joined_train.groupby(\"h3_index\")[columns_to_add].mean()\n",
    "dev_averages_hex = joined_dev.groupby(\"h3_index\")[columns_to_add].mean()\n",
    "test_averages_hex = joined_test.groupby(\"h3_index\")[columns_to_add].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed h3 regions to vectors. Use srai library to train spatial embeddings on train dataset with chosen embedder type (i.e. Hex2Vec, GeoVex ) and use it to get embeddings for hexagons in train, dev and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_regions_train = ring_buffer_h3_regions_gdf(regions_train, 2)\n",
    "buffered_regions_dev = ring_buffer_h3_regions_gdf(regions_dev, 2)\n",
    "buffered_regions_test = ring_buffer_h3_regions_gdf(regions_test, 2)\n",
    "\n",
    "\n",
    "osm_features = OSMPbfLoader().load(full_regions, HEX2VEC_FILTER)\n",
    "region_intersect_train = IntersectionJoiner().transform(buffered_regions_train, osm_features)\n",
    "\n",
    "\n",
    "embedder = Hex2VecEmbedder(embedder_hidden_sizes)\n",
    "neighbourhood = H3Neighbourhood(buffered_regions_train)\n",
    "\n",
    "# # for CCE and CE\n",
    "# neighbourhood = H3Neighbourhood(full_regions)\n",
    "# embedder = ContextualCountEmbedder(neighbourhood=neighbourhood,\n",
    "#   neighbourhood_distance=2,\n",
    "#   expected_output_features=HEX2VEC_FILTER,\n",
    "#   concatenate_vectors=True,\n",
    "#   count_subcategories=True)\n",
    "# embedder = CountEmbedder(expected_output_features=HEX2VEC_FILTER)\n",
    "\n",
    "# # for GV\n",
    "# embedder = GeoVexEmbedder(target_features=HEX2VEC_FILTER, neighbourhood_radius=2)\n",
    "# neighbourhood = H3Neighbourhood(full_regions)\n",
    "\n",
    "# Block neccessary only for H2V and GV\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    embedder.fit(\n",
    "        regions_gdf=buffered_regions_train,\n",
    "        features_gdf=osm_features,\n",
    "        joint_gdf=region_intersect_train,\n",
    "        neighbourhood=neighbourhood,\n",
    "        trainer_kwargs={\"max_epochs\": 10, \"accelerator\": device},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = embedder.transform(\n",
    "    regions_gdf=buffered_regions_train,\n",
    "    features_gdf=osm_features,\n",
    "    joint_gdf=region_intersect_train,\n",
    ")\n",
    "embeddings_train[\"h3\"] = embeddings_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_features_dev = OSMPbfLoader().load(buffered_regions_dev, HEX2VEC_FILTER)\n",
    "osm_features_test = OSMPbfLoader().load(buffered_regions_test, HEX2VEC_FILTER)\n",
    "\n",
    "region_intersect_dev = IntersectionJoiner().transform(buffered_regions_dev, osm_features_dev)\n",
    "region_intersect_test = IntersectionJoiner().transform(buffered_regions_test, osm_features_test)\n",
    "\n",
    "embeddings_dev = embedder.transform(\n",
    "    regions_gdf=buffered_regions_dev,\n",
    "    features_gdf=osm_features_dev,\n",
    "    joint_gdf=region_intersect_dev,\n",
    ")\n",
    "embeddings_dev[\"h3\"] = embeddings_dev.index\n",
    "\n",
    "embeddings_test = embedder.transform(\n",
    "    regions_gdf=buffered_regions_test,\n",
    "    features_gdf=osm_features_test,\n",
    "    joint_gdf=region_intersect_test,\n",
    ")\n",
    "embeddings_test[\"h3\"] = embeddings_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = embeddings_train.merge(\n",
    "    train_averages_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merged_dev = embeddings_dev.merge(\n",
    "    dev_averages_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merged_test = embeddings_test.merge(\n",
    "    test_averages_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merge_columns = [col for col in merged_train.columns if col not in ([\"h3\"] + [airbnb.target])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine numerical columns with the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns(row: gpd.GeoSeries) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate embedding values together.\n",
    "\n",
    "    Args:\n",
    "        row (gpd.GeoSeries): row of embeddings\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: concatenated embedding\n",
    "    \"\"\"\n",
    "    return np.concatenate([np.atleast_1d(val) for val in row.values]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get final version of data splits (X - embedding vector, X_h3_idx - h3 index, y - target value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_train[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_train[\"h3\"].values,\n",
    "        \"y\": merged_train[airbnb.target].values,\n",
    "    }\n",
    ")\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])\n",
    "\n",
    "\n",
    "dev_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_dev[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_dev[\"h3\"].values,\n",
    "        \"y\": merged_dev[airbnb.target].values,\n",
    "    }\n",
    ")\n",
    "dev_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])\n",
    "\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_test[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_test[\"h3\"].values,\n",
    "        \"y\": merged_test[airbnb.target].values,\n",
    "    }\n",
    ")\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = train_dataset[\"X\"].shape[1]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regression model\n",
    "\n",
    "Contains implementation of base model of regression.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RegressionBaseModel(nn.Module):  # type: ignore\n",
    "    \"\"\"\n",
    "    Regression base model.\n",
    "\n",
    "    Definition of Regression Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_size: int,\n",
    "        linear_sizes: Optional[list[int]] = None,\n",
    "        activation_function: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializaiton of regression module.\n",
    "\n",
    "        Args:\n",
    "            embeddings_size (int): size of input embedding\n",
    "            linear_sizes (Optional[list[int]], optional): sizes of linear layers inside module. \\\n",
    "                Defaults to [500, 1000].\n",
    "            activation_function (Optional[nn.Module], optional): activation function from torch.nn \\\n",
    "                Defaults to ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if linear_sizes is None:\n",
    "            linear_sizes = [500, 1000]\n",
    "        if activation_function is None:\n",
    "            activation_function = nn.ReLU()\n",
    "        self.model = torch.nn.Sequential()\n",
    "        previous_size = embeddings_size\n",
    "        for cnt, size in enumerate(linear_sizes):\n",
    "            self.model.add_module(f\"linear_{cnt}\", nn.Linear(previous_size, size))\n",
    "            self.model.add_module(f\"ReLU_{cnt}\", activation_function)\n",
    "            previous_size = size\n",
    "            if cnt % 2:\n",
    "                self.model.add_module(f\"dropout_{cnt}\", nn.Dropout(p=0.2))\n",
    "        self.model.add_module(\"linear_final\", nn.Linear(previous_size, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Vector data\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: target value\n",
    "        \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = RegressionBaseModel(embedding_size, linear_sizes=[50, 100, 50])\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(regression_model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "save_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = HexRegressionEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    dev_dataloader: DataLoader,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    evaluator: BaseEvaluator,\n",
    "    device: Union[str, torch.device] = \"cuda\",\n",
    "    save_dir: str = \"./\",\n",
    "    epochs: int = 50,\n",
    "    early_stopping_patience: int = 5,\n",
    ") -> tuple[float, nn.Module, dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Train a PyTorch model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "        dev_dataloader (DataLoader): DataLoader for the validation/dev dataset.\n",
    "        loss_fn (Callable): Loss function used for training (e.g., nn.MSELoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "        evaluator (Any): Object with a `_compute_metrics` method that accepts predicted and\n",
    "            target values (as numpy arrays) and returns a dictionary of metric results.\n",
    "        device (str or torch.device): The device on which to run the training (e.g., \"cuda\", \"cpu\").\n",
    "        save_dir (str): Directory path to save the best model weights.\n",
    "        epochs (int, optional): Maximum number of training epochs. Defaults to 50.\n",
    "        early_stopping_patience (int, optional): Number of epochs with no improvement on validation\n",
    "            loss before early stopping. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, nn.Module, Dict[str, float]]: A tuple containing:\n",
    "            - Final validation loss (float)\n",
    "            - The trained model (nn.Module)\n",
    "            - The last computed validation metrics (dict of metric name to value)\n",
    "    \"\"\"\n",
    "    stop_counter = 0\n",
    "    prev_eval_loss = np.inf\n",
    "    loss_eval: list[float] = []\n",
    "    loss_train: list[float] = []\n",
    "    metrics_results: list[dict[str, float]] = []\n",
    "    best_weights: Optional[dict] = None\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss_list = []\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch: {epoch}\", total=len(train_dataloader)):\n",
    "            inputs = batch[\"X\"].to(device)\n",
    "            labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss_list.append(loss.item())\n",
    "\n",
    "        logging.info(f\"Epoch [{epoch+1}/{epochs}], avg_loss: {np.mean(batch_loss_list):.4f}\")\n",
    "        loss_train.append(np.mean(batch_loss_list))\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        metrics_per_batch: list[dict[str, float]] = []\n",
    "        batch_eval_loss: list[float] = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(\n",
    "                enumerate(dev_dataloader), desc=\"Evaluation\", total=len(dev_dataloader)\n",
    "            ):\n",
    "                inputs = batch[\"X\"].to(device)\n",
    "                labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                batch_eval_loss.append(float(loss.item()))\n",
    "\n",
    "                metrics = evaluator._compute_metrics(\n",
    "                    np.asarray(outputs.cpu()), np.asarray(labels.cpu())\n",
    "                )\n",
    "                metrics_per_batch.append({\"Batch\": i, **metrics})\n",
    "\n",
    "        mean_metrics = {\n",
    "            key: np.mean([batch[key] for batch in metrics_per_batch])\n",
    "            for key in metrics_per_batch[0].keys()\n",
    "            if key != \"Batch\"\n",
    "        }\n",
    "        metrics_results.append(mean_metrics)\n",
    "        val_loss = np.mean(batch_eval_loss)\n",
    "        loss_eval.append(val_loss)\n",
    "        logging.info(f\"Evaluation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss >= prev_eval_loss:\n",
    "            stop_counter += 1\n",
    "            if stop_counter == early_stopping_patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "                break\n",
    "        else:\n",
    "            stop_counter = 0\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "        prev_eval_loss = val_loss\n",
    "\n",
    "    # Load best weights\n",
    "    if best_weights:\n",
    "        model.load_state_dict(best_weights)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"hex_regression_best_model.pkl\"))\n",
    "    return val_loss, model, metrics_results[-1] if metrics_results else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, regression_model, metrics = train(\n",
    "    model=regression_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    dev_dataloader=dev_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    evaluator=evaluator,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    epochs=epochs,\n",
    "    early_stopping_patience=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.eval()\n",
    "h3_indexes = []\n",
    "xy_points = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Predicting...\", total=len(test_dataloader)):\n",
    "        inputs = batch[\"X\"].to(device)\n",
    "        indexes = batch[\"X_h3_idx\"]\n",
    "        points = batch[\"point\"] if \"point\" in batch else [\"\" for _ in indexes]\n",
    "        outputs = regression_model(inputs)\n",
    "        h3_indexes.extend(indexes)\n",
    "        xy_points.extend(points)\n",
    "        all_predictions.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(\n",
    "    dataset=airbnb,\n",
    "    predictions=all_predictions,\n",
    "    region_ids=h3_indexes,\n",
    "    log_metrics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_label = [test_dataset[i][\"y\"] for i in range(len(test_dataset))]\n",
    "original_hexes = [test_dataset[i][\"X_h3_idx\"] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = h3_to_geoseries(\n",
    "    h3_indexes,\n",
    ")\n",
    "preds_gdf = gpd.GeoDataFrame(geometry=polygons)\n",
    "preds_gdf.crs = {\"init\": \"epsg:4326\"}\n",
    "preds_gdf[\"price\"] = [tensor.item() for tensor in all_predictions]\n",
    "preds_gdf[\"region_id\"] = h3_indexes\n",
    "preds_gdf.index = preds_gdf[\"region_id\"]\n",
    "\n",
    "original_polygons = h3_to_geoseries(original_hexes)\n",
    "original_gdf = gpd.GeoDataFrame(geometry=[Polygon(polygon) for polygon in original_polygons])\n",
    "original_gdf.crs = {\"init\": \"epsg:4326\"}\n",
    "original_gdf[\"price\"] = [tensor.item() for tensor in original_label]\n",
    "original_gdf[\"region_id\"] = original_hexes\n",
    "original_gdf.index = original_gdf[\"region_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionalizer = H3Regionalizer(resolution=resolution)\n",
    "regions = regionalizer.transform(original_gdf)\n",
    "plot_numeric_data(regions, \"price\", original_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_data(regions, \"price\", preds_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
