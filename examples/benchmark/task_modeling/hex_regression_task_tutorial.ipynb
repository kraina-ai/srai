{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from typing import Optional\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from srai.benchmark import HexRegressionEvaluator\n",
    "from srai.datasets import AirbnbMulticityDataset\n",
    "from srai.embedders import Hex2VecEmbedder  # noqa: F401\n",
    "from srai.h3 import h3_to_geoseries\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders.osm_loaders import OSMPbfLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.plotting import plot_numeric_data\n",
    "from srai.regionalizers import H3Regionalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resolution = 9\n",
    "embedder_hidden_sizes = [150, 75, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "regionalizer = H3Regionalizer(resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = AirbnbMulticityDataset()\n",
    "ds = airbnb.load(version=str(resolution), hf_token=os.getenv(\"HF_TOKEN\"))\n",
    "train, test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dev split from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = airbnb.train_test_split_bucket_regression(test_size=0.1, dev=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about available categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb.categorical_columns, airbnb.numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = train.copy()\n",
    "dev_ = dev.copy()\n",
    "test_ = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get h3 indexes for data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train = regionalizer.transform(train_)\n",
    "joined_train = gpd.sjoin(train_, regions_train, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_train.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)\n",
    "\n",
    "regions_dev = regionalizer.transform(dev_)\n",
    "joined_dev = gpd.sjoin(dev_, regions_dev, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_dev.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)\n",
    "\n",
    "\n",
    "regions_test = regionalizer.transform(test_)\n",
    "joined_test = gpd.sjoin(test_, regions_test, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_test.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(regions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = airbnb.numerical_columns + [airbnb.target]\n",
    "joined_train[airbnb.numerical_columns] = scaler.fit_transform(\n",
    "    joined_train[airbnb.numerical_columns]\n",
    ")\n",
    "train_averages_hex = joined_train.groupby(\"h3_index\")[columns_to_add].mean()\n",
    "\n",
    "joined_dev[airbnb.numerical_columns] = scaler.transform(joined_dev[airbnb.numerical_columns])\n",
    "joined_test[airbnb.numerical_columns] = scaler.transform(joined_test[airbnb.numerical_columns])\n",
    "\n",
    "dev_averages_hex = joined_dev.groupby(\"h3_index\")[columns_to_add].mean()\n",
    "test_averages_hex = joined_test.groupby(\"h3_index\")[columns_to_add].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed h3 regions to vectors. Use srai library to train spatial embeddings on train dataset with chosen embedder type (i.e. Hex2Vec, GeoVex ) and use it to get embeddings for hexagons in train, dev and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_features = OSMPbfLoader().load(regions_train, HEX2VEC_FILTER)\n",
    "embedder = Hex2VecEmbedder(embedder_hidden_sizes)\n",
    "region_intersect_train = IntersectionJoiner().transform(regions_train, osm_features)\n",
    "neighbourhood = H3Neighbourhood(regions_train)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    embedder.fit(\n",
    "        regions_gdf=regions_train,\n",
    "        features_gdf=osm_features,\n",
    "        joint_gdf=region_intersect_train,\n",
    "        neighbourhood=neighbourhood,\n",
    "        trainer_kwargs={\"max_epochs\": 10, \"accelerator\": device},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = embedder.transform(\n",
    "    regions_gdf=regions_train, features_gdf=osm_features, joint_gdf=region_intersect_train\n",
    ")\n",
    "embeddings_train[\"h3\"] = embeddings_train.index\n",
    "\n",
    "osm_features_dev = OSMPbfLoader().load(regions_dev, HEX2VEC_FILTER)\n",
    "osm_features_test = OSMPbfLoader().load(regions_test, HEX2VEC_FILTER)\n",
    "\n",
    "region_intersect_dev = IntersectionJoiner().transform(regions_dev, osm_features_dev)\n",
    "region_intersect_test = IntersectionJoiner().transform(regions_test, osm_features_test)\n",
    "\n",
    "embeddings_dev = embedder.transform(\n",
    "    regions_gdf=regions_dev, features_gdf=osm_features_dev, joint_gdf=region_intersect_dev\n",
    ")\n",
    "embeddings_dev[\"h3\"] = embeddings_dev.index\n",
    "\n",
    "embeddings_test = embedder.transform(\n",
    "    regions_gdf=regions_test, features_gdf=osm_features_test, joint_gdf=region_intersect_test\n",
    ")\n",
    "embeddings_test[\"h3\"] = embeddings_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = embeddings_train.merge(\n",
    "    train_averages_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merged_dev = embeddings_dev.merge(\n",
    "    dev_averages_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merged_test = embeddings_test.merge(\n",
    "    test_averages_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merge_columns = [col for col in merged_train.columns if col not in ([\"h3\"] + [airbnb.target])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine numerical columns with the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns(row: gpd.GeoSeries) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate embedding values together.\n",
    "\n",
    "    Args:\n",
    "        row (gpd.GeoSeries): row of embeddings\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: concatenated embedding\n",
    "    \"\"\"\n",
    "    return np.concatenate([np.atleast_1d(val) for val in row.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get final version of data splits (X - embedding vector, X_h3_idx - h3 index, y - target value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_train[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_train[\"h3\"].values,\n",
    "        \"y\": merged_train[airbnb.target].values,\n",
    "    }\n",
    ")\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])\n",
    "\n",
    "\n",
    "dev_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_dev[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_dev[\"h3\"].values,\n",
    "        \"y\": merged_dev[airbnb.target].values,\n",
    "    }\n",
    ")\n",
    "dev_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])\n",
    "\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_test[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_test[\"h3\"].values,\n",
    "        \"y\": merged_test[airbnb.target].values,\n",
    "    }\n",
    ")\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = train_dataset[\"X\"].shape[1]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regression model\n",
    "\n",
    "Contains implementation of base model of regression.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RegressionBaseModel(nn.Module):  # type: ignore\n",
    "    \"\"\"\n",
    "    Regression base model.\n",
    "\n",
    "    Definition of Regression Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_size: int,\n",
    "        linear_sizes: Optional[list[int]] = None,\n",
    "        activation_function: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializaiton of regression module.\n",
    "\n",
    "        Args:\n",
    "            embeddings_size (int): size of input embedding\n",
    "            linear_sizes (Optional[list[int]], optional): sizes of linear layers inside module. \\\n",
    "                Defaults to [500, 1000].\n",
    "            activation_function (Optional[nn.Module], optional): activation function from torch.nn \\\n",
    "                Defaults to ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if linear_sizes is None:\n",
    "            linear_sizes = [500, 1000]\n",
    "        if activation_function is None:\n",
    "            activation_function = nn.ReLU()\n",
    "        self.model = torch.nn.Sequential()\n",
    "        previous_size = embeddings_size\n",
    "        for cnt, size in enumerate(linear_sizes):\n",
    "            self.model.add_module(f\"linear_{cnt}\", nn.Linear(previous_size, size))\n",
    "            self.model.add_module(f\"ReLU_{cnt}\", activation_function)\n",
    "            previous_size = size\n",
    "            if cnt % 2:\n",
    "                self.model.add_module(f\"dropout_{cnt}\", nn.Dropout(p=0.2))\n",
    "        self.model.add_module(\"linear_final\", nn.Linear(previous_size, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Vector data\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: target value\n",
    "        \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = RegressionBaseModel(embedding_size)\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(regression_model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "save_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = HexRegressionEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_counter = 0\n",
    "prev_eval_loss = np.inf  # init to infinity\n",
    "loss_eval = []\n",
    "loss_train = []\n",
    "metrics_results = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_loss_list = []\n",
    "    regression_model.train()\n",
    "    for batch in tqdm(\n",
    "        train_dataloader,\n",
    "        desc=f\"Epoch: {epoch}\",\n",
    "        total=len(train_dataloader),\n",
    "    ):\n",
    "        inputs = batch[\"X\"].to(device)\n",
    "        labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "        outputs = regression_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss_list.append(loss.item())\n",
    "\n",
    "    logging.info(f\"Epoch [{epoch+1}/{epochs}], avg_loss: {np.mean(batch_loss_list):.4f}\")\n",
    "    loss_train.append(np.mean(batch_loss_list))\n",
    "\n",
    "    regression_model.eval()\n",
    "    metrics_per_batch = []\n",
    "    batch_eval_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(\n",
    "            enumerate(dev_dataloader),\n",
    "            desc=\"Evaluation\",\n",
    "            total=len(dev_dataloader),\n",
    "        ):\n",
    "            inputs = batch[\"X\"].to(device)\n",
    "            labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "            outputs = regression_model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            batch_eval_loss.append(float(loss.item()))\n",
    "\n",
    "            metrics = evaluator._compute_metrics(np.asarray(outputs), np.asarray(labels))\n",
    "            metrics_per_batch.append({\"Batch\": i, **metrics})\n",
    "\n",
    "    mean_metrics = {\n",
    "        key: np.mean([batch[key] for batch in metrics_per_batch])\n",
    "        for key in metrics_per_batch[0].keys()\n",
    "        if key != \"Batch\"\n",
    "    }\n",
    "    metrics_results.append(mean_metrics)\n",
    "    loss_eval.append(np.mean(batch_eval_loss))\n",
    "    logging.info(f\"Evaluation loss: {loss_eval[-1]:.4f}\")\n",
    "    # early_stopping\n",
    "    if loss_eval[-1] >= prev_eval_loss:\n",
    "        stop_counter += 1\n",
    "        if stop_counter == 5:\n",
    "            logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "            best_weights = copy.deepcopy(regression_model.state_dict())\n",
    "            break\n",
    "        prev_eval_loss = loss_eval[-1]\n",
    "if stop_counter == 5:\n",
    "    regression_model.load_state_dict(best_weights)\n",
    "\n",
    "torch.save(regression_model.state_dict(), os.path.join(save_dir, \"airbnb_best_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.eval()\n",
    "h3_indexes = []\n",
    "xy_points = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Predicting...\", total=len(test_dataloader)):\n",
    "        inputs = batch[\"X\"].to(device)\n",
    "        indexes = batch[\"X_h3_idx\"]\n",
    "        points = batch[\"point\"] if \"point\" in batch else [\"\" for _ in indexes]\n",
    "        outputs = regression_model(inputs)\n",
    "        h3_indexes.extend(indexes)\n",
    "        xy_points.extend(points)\n",
    "        all_predictions.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(\n",
    "    dataset=airbnb, predictions=all_predictions, region_ids=h3_indexes, log_metrics=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_label = [test_dataset[i][\"y\"] for i in range(len(test_dataset))]\n",
    "original_hexes = [test_dataset[i][\"X_h3_idx\"] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = h3_to_geoseries(\n",
    "    h3_indexes,\n",
    ")\n",
    "preds_gdf = gpd.GeoDataFrame(geometry=polygons)\n",
    "preds_gdf.crs = {\"init\": \"epsg:4326\"}\n",
    "preds_gdf[\"price\"] = [tensor.item() for tensor in all_predictions]\n",
    "preds_gdf[\"region_id\"] = h3_indexes\n",
    "preds_gdf.index = preds_gdf[\"region_id\"]\n",
    "\n",
    "original_polygons = h3_to_geoseries(original_hexes)\n",
    "original_gdf = gpd.GeoDataFrame(geometry=[Polygon(polygon) for polygon in original_polygons])\n",
    "original_gdf.crs = {\"init\": \"epsg:4326\"}\n",
    "original_gdf[\"price\"] = [tensor.item() for tensor in original_label]\n",
    "original_gdf[\"region_id\"] = original_hexes\n",
    "original_gdf.index = original_gdf[\"region_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionalizer = H3Regionalizer(resolution=resolution)\n",
    "regions = regionalizer.transform(original_gdf)\n",
    "plot_numeric_data(regions, \"price\", original_gdf)\n",
    "# CO JEST NIE TAK??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_data(regions, \"price\", preds_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
