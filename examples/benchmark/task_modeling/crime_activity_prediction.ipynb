{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from shapely.geometry import Polygon, box\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from srai.benchmark import BaseEvaluator, HexRegressionEvaluator\n",
    "from srai.datasets import (\n",
    "    ChicagoCrimeDataset,\n",
    ")\n",
    "from srai.embedders import Hex2VecEmbedder\n",
    "from srai.h3 import h3_to_geoseries\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders.osm_loaders import OSMPbfLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.plotting import plot_numeric_data\n",
    "from srai.regionalizers import H3Regionalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resolution = 8\n",
    "embedder_hidden_sizes = [150, 75, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "regionalizer = H3Regionalizer(resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes = ChicagoCrimeDataset()\n",
    "ds = crimes.load(version=str(resolution))\n",
    "train, test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dev split from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = crimes.train_test_split_bucket_regression(\n",
    "    test_size=0.1, dev=True, resolution=resolution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_h3 = crimes.get_h3_with_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about available categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.categorical_columns, crimes.numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = train.copy()\n",
    "dev_ = dev.copy()\n",
    "test_ = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get h3 indexes for data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train = regionalizer.transform(train_)\n",
    "full_geometry = regions_train.unary_union.convex_hull.buffer(0.1)\n",
    "\n",
    "full_regions = regionalizer.transform(\n",
    "    gpd.GeoDataFrame([\"full\"], geometry=[full_geometry]).set_crs(regions_train.crs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_train = regionalizer.transform(train_)\n",
    "joined_train = gpd.sjoin(train_, regions_train, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_train.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)\n",
    "\n",
    "regions_dev = regionalizer.transform(dev_)\n",
    "joined_dev = gpd.sjoin(dev_, regions_dev, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_dev.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)\n",
    "\n",
    "\n",
    "regions_test = regionalizer.transform(test_)\n",
    "joined_test = gpd.sjoin(test_, regions_test, how=\"left\", predicate=\"within\")  # noqa: E501\n",
    "joined_test.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group to hex (results in count of crimes per hex)\n",
    "train_counts_per_hex = joined_train.groupby(\"h3_index\").size().reset_index(name=\"count\")\n",
    "dev_counts_per_hex = joined_dev.groupby(\"h3_index\").size().reset_index(name=\"count\")\n",
    "test_counts_per_hex = joined_test.groupby(\"h3_index\").size().reset_index(name=\"count\")\n",
    "\n",
    "# scale the hex-level counts using MinMaxScaler\n",
    "\n",
    "train_counts_per_hex[\"count\"] = scaler.fit_transform(train_counts_per_hex[[\"count\"]])\n",
    "dev_counts_per_hex[\"count\"] = scaler.transform(dev_counts_per_hex[[\"count\"]])\n",
    "dev_counts_per_hex[\"count\"] = np.clip(dev_counts_per_hex[\"count\"], 0, 1)\n",
    "test_counts_per_hex[\"count\"] = scaler.transform(test_counts_per_hex[[\"count\"]])\n",
    "test_counts_per_hex[\"count\"] = np.clip(test_counts_per_hex[\"count\"], 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed h3 regions to vectors. Use srai library to train spatial embeddings on train dataset with chosen embedder type (i.e. Hex2Vec, GeoVex ) and use it to get embeddings for hexagons in train, dev and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.h3 import ring_buffer_h3_regions_gdf\n",
    "\n",
    "buffered_regions_train = ring_buffer_h3_regions_gdf(regions_train, 2)\n",
    "buffered_regions_dev = ring_buffer_h3_regions_gdf(regions_dev, 2)\n",
    "buffered_regions_test = ring_buffer_h3_regions_gdf(regions_test, 2)\n",
    "\n",
    "\n",
    "osm_features = OSMPbfLoader().load(full_regions, HEX2VEC_FILTER)\n",
    "region_intersect_train = IntersectionJoiner().transform(buffered_regions_train, osm_features)\n",
    "\n",
    "# # For CCE or CE usage\n",
    "# neighbourhood = H3Neighbourhood(full_regions)\n",
    "# embedder = ContextualCountEmbedder(neighbourhood=neighbourhood,\n",
    "#                                    neighbourhood_distance=2,\n",
    "#                                    expected_output_features=HEX2VEC_FILTER,\n",
    "#                                     concatenate_vectors=True,\n",
    "#                                     count_subcategories=True)\n",
    "# embedder = CountEmbedder(expected_output_features=HEX2VEC_FILTER)\n",
    "\n",
    "# # For H2V usage\n",
    "embedder = Hex2VecEmbedder(embedder_hidden_sizes)\n",
    "neighbourhood = H3Neighbourhood(buffered_regions_train)\n",
    "\n",
    "\n",
    "# # For GV usage\n",
    "# embedder = GeoVexEmbedder(target_features=HEX2VEC_FILTER, neighbourhood_radius=2)\n",
    "# neighbourhood = H3Neighbourhood(full_regions)\n",
    "\n",
    "# Needed for H2V and GV. Comment fitting block out for CCE and CE\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    embedder.fit(\n",
    "        regions_gdf=buffered_regions_train,\n",
    "        features_gdf=osm_features,\n",
    "        joint_gdf=region_intersect_train,\n",
    "        neighbourhood=neighbourhood,\n",
    "        trainer_kwargs={\"max_epochs\": 10, \"accelerator\": device},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = embedder.transform(\n",
    "    regions_gdf=buffered_regions_train,\n",
    "    features_gdf=osm_features,\n",
    "    joint_gdf=region_intersect_train,\n",
    ")\n",
    "embeddings_train[\"h3\"] = embeddings_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_features_dev = OSMPbfLoader().load(buffered_regions_dev, HEX2VEC_FILTER)\n",
    "osm_features_test = OSMPbfLoader().load(buffered_regions_test, HEX2VEC_FILTER)\n",
    "\n",
    "region_intersect_dev = IntersectionJoiner().transform(buffered_regions_dev, osm_features_dev)\n",
    "region_intersect_test = IntersectionJoiner().transform(buffered_regions_test, osm_features_test)\n",
    "\n",
    "embeddings_dev = embedder.transform(\n",
    "    regions_gdf=buffered_regions_dev,\n",
    "    features_gdf=osm_features_dev,\n",
    "    joint_gdf=region_intersect_dev,\n",
    ")\n",
    "embeddings_dev[\"h3\"] = embeddings_dev.index\n",
    "\n",
    "embeddings_test = embedder.transform(\n",
    "    regions_gdf=buffered_regions_test,\n",
    "    features_gdf=osm_features_test,\n",
    "    joint_gdf=region_intersect_test,\n",
    ")\n",
    "embeddings_test[\"h3\"] = embeddings_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = embeddings_train.merge(\n",
    "    train_counts_per_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merged_dev = embeddings_dev.merge(\n",
    "    dev_counts_per_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merged_test = embeddings_test.merge(\n",
    "    test_counts_per_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "\n",
    "merge_columns = [\n",
    "    col for col in merged_train.columns if col not in ([\"h3\"] + [crimes.target] + [\"h3_index\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine numerical columns with the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns(row: gpd.GeoSeries) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate embedding values together.\n",
    "\n",
    "    Args:\n",
    "        row (gpd.GeoSeries): row of embeddings\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: concatenated embedding\n",
    "    \"\"\"\n",
    "    return np.concatenate([np.atleast_1d(val) for val in row.values]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get final version of data splits (X - embedding vector, X_h3_idx - h3 index, y - target value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_train[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_train[\"h3\"].values,\n",
    "        \"y\": merged_train[crimes.target].values,\n",
    "    }\n",
    ")\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])\n",
    "\n",
    "\n",
    "dev_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_dev[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_dev[\"h3\"].values,\n",
    "        \"y\": merged_dev[crimes.target].values,\n",
    "    }\n",
    ")\n",
    "dev_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])\n",
    "\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"X\": merged_test[merge_columns].apply(concat_columns, axis=1).values,\n",
    "        \"X_h3_idx\": merged_test[\"h3\"].values,\n",
    "        \"y\": merged_test[crimes.target].values,\n",
    "    }\n",
    ")\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"X\", \"X_h3_idx\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = train_dataset[\"X\"].shape[1]\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regression model\n",
    "\n",
    "Contains implementation of base model of regression.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RegressionBaseModel(nn.Module):  # type: ignore\n",
    "    \"\"\"\n",
    "    Regression base model.\n",
    "\n",
    "    Definition of Regression Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_size: int,\n",
    "        linear_sizes: Optional[list[int]] = None,\n",
    "        activation_function: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializaiton of regression module.\n",
    "\n",
    "        Args:\n",
    "            embeddings_size (int): size of input embedding\n",
    "            linear_sizes (Optional[list[int]], optional): sizes of linear layers inside module. \\\n",
    "                Defaults to [500, 1000].\n",
    "            activation_function (Optional[nn.Module], optional): activation function from torch.nn \\\n",
    "                Defaults to ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if linear_sizes is None:\n",
    "            linear_sizes = [500, 1000]\n",
    "        if activation_function is None:\n",
    "            activation_function = nn.ReLU()\n",
    "        self.model = torch.nn.Sequential()\n",
    "        previous_size = embeddings_size\n",
    "        for cnt, size in enumerate(linear_sizes):\n",
    "            self.model.add_module(f\"linear_{cnt}\", nn.Linear(previous_size, size))\n",
    "            self.model.add_module(f\"ReLU_{cnt}\", activation_function)\n",
    "            previous_size = size\n",
    "            if cnt % 2:\n",
    "                self.model.add_module(f\"dropout_{cnt}\", nn.Dropout(p=0.2))\n",
    "        self.model.add_module(\"linear_final\", nn.Linear(previous_size, 1))\n",
    "        self.model.add_module(\"sigmoid_output\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Vector data\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: target value\n",
    "        \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = RegressionBaseModel(\n",
    "    embedding_size, linear_sizes=[50, 100, 50], activation_function=nn.Sigmoid()\n",
    ")\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(regression_model.parameters(), lr=0.001)\n",
    "epochs = 70\n",
    "batch_size = 32\n",
    "save_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = HexRegressionEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    dev_dataloader: DataLoader,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    optimizer: Optimizer,\n",
    "    evaluator: BaseEvaluator,\n",
    "    device: Union[str, torch.device] = \"cuda\",\n",
    "    save_dir: str = \"./\",\n",
    "    epochs: int = 50,\n",
    "    early_stopping_patience: int = 5,\n",
    ") -> tuple[float, nn.Module, dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model with early stopping and evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_dataloader (DataLoader): DataLoader for training data.\n",
    "        dev_dataloader (DataLoader): DataLoader for validation data.\n",
    "        loss_fn (Callable): Loss function used for training (e.g., nn.MSELoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "        evaluator (Any): Object with a `_compute_metrics` method that accepts predicted and\n",
    "            target values (as numpy arrays) and returns a dictionary of metric results.\n",
    "        device (str or torch.device): Device to run the model on ('cpu' or 'cuda').\n",
    "        save_dir (str): Directory where the best model will be saved.\n",
    "        epochs (int, optional): Number of training epochs. Defaults to 50.\n",
    "        early_stopping_patience (int, optional): Number of evaluations without improvement\n",
    "            before early stopping. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, nn.Module, Dict[str, float]]:\n",
    "            - Final validation loss,\n",
    "            - Trained model (with best weights loaded),\n",
    "            - Final evaluation metrics dictionary.\n",
    "    \"\"\"\n",
    "    stop_counter = 0\n",
    "    prev_eval_loss = np.inf\n",
    "    loss_eval: list[float] = []\n",
    "    loss_train: list[float] = []\n",
    "    metrics_results: list[dict[str, float]] = []\n",
    "    best_weights: Optional[dict] = None\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss_list = []\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch: {epoch}\", total=len(train_dataloader)):\n",
    "            inputs = batch[\"X\"].to(device)\n",
    "            labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss_list.append(loss.item())\n",
    "\n",
    "        logging.info(f\"Epoch [{epoch+1}/{epochs}], avg_loss: {np.mean(batch_loss_list):.4f}\")\n",
    "        loss_train.append(np.mean(batch_loss_list))\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        metrics_per_batch: list[dict[str, float]] = []\n",
    "        batch_eval_loss: list[float] = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(\n",
    "                enumerate(dev_dataloader), desc=\"Evaluation\", total=len(dev_dataloader)\n",
    "            ):\n",
    "                inputs = batch[\"X\"].to(device)\n",
    "                labels = batch[\"y\"].to(device).reshape(-1, 1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                batch_eval_loss.append(float(loss.item()))\n",
    "\n",
    "                metrics = evaluator._compute_metrics(\n",
    "                    np.asarray(outputs.cpu()), np.asarray(labels.cpu())\n",
    "                )\n",
    "                metrics_per_batch.append({\"Batch\": i, **metrics})\n",
    "\n",
    "        mean_metrics = {\n",
    "            key: np.mean([batch[key] for batch in metrics_per_batch])\n",
    "            for key in metrics_per_batch[0].keys()\n",
    "            if key != \"Batch\"\n",
    "        }\n",
    "        metrics_results.append(mean_metrics)\n",
    "        val_loss = np.mean(batch_eval_loss)\n",
    "        loss_eval.append(val_loss)\n",
    "        logging.info(f\"Evaluation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss >= prev_eval_loss:\n",
    "            stop_counter += 1\n",
    "            if stop_counter == early_stopping_patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "                break\n",
    "        else:\n",
    "            stop_counter = 0\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "        prev_eval_loss = val_loss\n",
    "\n",
    "    # Load best weights\n",
    "    if best_weights:\n",
    "        model.load_state_dict(best_weights)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"CAP_best_model.pkl\"))\n",
    "    return val_loss, model, metrics_results[-1] if metrics_results else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, regression_model, metrics = train(\n",
    "    model=regression_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    dev_dataloader=dev_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    evaluator=evaluator,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    epochs=epochs,\n",
    "    early_stopping_patience=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.to(device)\n",
    "regression_model.eval()\n",
    "h3_indexes = []\n",
    "xy_points = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Predicting...\", total=len(test_dataloader)):\n",
    "        inputs = batch[\"X\"].to(device)\n",
    "        indexes = batch[\"X_h3_idx\"]\n",
    "        points = batch[\"point\"] if \"point\" in batch else [\"\" for _ in indexes]\n",
    "        outputs = regression_model(inputs)\n",
    "        h3_indexes.extend(indexes)\n",
    "        xy_points.extend(points)\n",
    "        all_predictions.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.resolution, crimes.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(\n",
    "    dataset=crimes,\n",
    "    predictions=all_predictions,\n",
    "    region_ids=h3_indexes,\n",
    "    log_metrics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_label = [test_dataset[i][\"y\"] for i in range(len(test_dataset))]\n",
    "original_hexes = [test_dataset[i][\"X_h3_idx\"] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_bbox = box(-87.9401, 41.6445, -87.5237, 42.0230)\n",
    "# philadelphia_bbox = box(-75.2803, 39.8670, -74.9558, 40.1376)\n",
    "# sf_box=box(-123.173825, 37.639830, -122.281780, 37.929824)\n",
    "polygons = h3_to_geoseries(h3_indexes)\n",
    "preds_gdf = gpd.GeoDataFrame(geometry=polygons)\n",
    "preds_gdf.crs = {\"init\": \"epsg:4326\"}\n",
    "preds_gdf[\"intensity\"] = [t.item() for t in all_predictions]\n",
    "preds_gdf[\"region_id\"] = h3_indexes\n",
    "preds_gdf.index = preds_gdf[\"region_id\"]\n",
    "\n",
    "# Original labeled hexes\n",
    "original_polygons = h3_to_geoseries(original_hexes)\n",
    "original_gdf = gpd.GeoDataFrame(geometry=[Polygon(polygon) for polygon in original_polygons])\n",
    "original_gdf.crs = {\"init\": \"epsg:4326\"}\n",
    "original_gdf[\"intensity\"] = [t.item() for t in original_label]\n",
    "original_gdf[\"region_id\"] = original_hexes\n",
    "original_gdf.index = original_gdf[\"region_id\"]\n",
    "\n",
    "# Generate H3 regions\n",
    "regionalizer = H3Regionalizer(resolution=resolution)\n",
    "regions = regionalizer.transform(original_gdf)\n",
    "\n",
    "# Filter to chosen region using bbox\n",
    "regions_to_plot = regions[regions.intersects(chicago_bbox)]\n",
    "original_gdf = original_gdf[original_gdf.geometry.intersects(chicago_bbox)]\n",
    "preds_gdf = preds_gdf[preds_gdf.geometry.intersects(chicago_bbox)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_data(regions_to_plot, \"intensity\", original_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_data(regions_to_plot, \"intensity\", preds_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
