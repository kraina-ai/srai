{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import choice\n",
    "from string import ascii_lowercase, digits\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import box\n",
    "\n",
    "from srai.constants import FEATURES_INDEX\n",
    "from srai.embedders.count_embedder import CountEmbedder\n",
    "from srai.h3 import ring_buffer_h3_regions_gdf\n",
    "from srai.joiners.intersection_joiner import IntersectionJoiner\n",
    "from srai.regionalizers.h3_regionalizer import H3Regionalizer\n",
    "from srai.regionalizers.s2_regionalizer import S2Regionalizer\n",
    "\n",
    "H3_RESOLUTION = 5\n",
    "# TODO: increase after rewriting s2 regionalizer\n",
    "S2_RESOLUTION = 6  # 13\n",
    "H3_DISTANCE = 6\n",
    "\n",
    "chars = ascii_lowercase + digits\n",
    "columns = [\"\".join(choice(chars) for _ in range(8)) for _ in range(100)]\n",
    "values = [\"\".join(choice(chars) for _ in range(8)) for _ in range(100)]\n",
    "\n",
    "area = gpd.GeoDataFrame(geometry=[box(5.818355, 46.037418, 24.363277, 52.769854)], crs=4326)\n",
    "\n",
    "h3_regions = H3Regionalizer(resolution=H3_RESOLUTION).transform(area)\n",
    "print(f\"H3 regions: {len(h3_regions)}\")\n",
    "buffered_h3_regions = ring_buffer_h3_regions_gdf(h3_regions, H3_DISTANCE)\n",
    "print(f\"Buffered H3 regions: {len(buffered_h3_regions)}\")\n",
    "\n",
    "s2_regions = S2Regionalizer(resolution=S2_RESOLUTION).transform(area)\n",
    "print(f\"S2 regions: {len(s2_regions)}\")\n",
    "\n",
    "data = np.full((len(s2_regions), len(columns)), None)\n",
    "for i in range(len(s2_regions)):\n",
    "    data[i, random.randint(0, len(columns) - 1)] = random.choice(values)\n",
    "\n",
    "s2_regions[columns] = data\n",
    "s2_regions.index.rename(FEATURES_INDEX, inplace=True)\n",
    "\n",
    "joint = IntersectionJoiner().transform(buffered_h3_regions, s2_regions)\n",
    "print(f\"Joint: {len(joint)}\")\n",
    "\n",
    "count_embeddings = CountEmbedder(\n",
    "    count_subcategories=True,\n",
    ").transform(buffered_h3_regions, s2_regions, joint)\n",
    "\n",
    "count_embeddings\n",
    "\n",
    "# embeddings = ContextualCountEmbedder(\n",
    "#     neighbourhood=H3Neighbourhood(),\n",
    "#     neighbourhood_distance=H3_DISTANCE,\n",
    "#     count_subcategories=True,\n",
    "#     concatenate_vectors=False,\n",
    "# ).transform(buffered_h3_regions, s2_regions, joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_embeddings.parquet_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dense array size:\", count_embeddings.to_dataframe().values.nbytes / 1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = count_embeddings.columns\n",
    "\n",
    "csr_rows = []\n",
    "csr_cols = []\n",
    "csr_values = []\n",
    "\n",
    "print(\"Columns:\", columns)\n",
    "for batch in count_embeddings.to_duckdb(with_row_number=True).fetch_arrow_reader():\n",
    "    pl_df = pl.from_arrow(batch)\n",
    "    for row in pl_df.iter_rows(named=True):\n",
    "        row_idx = row[\"row_number\"] - 1  # duckdb row_number starts from 1\n",
    "        for col_idx, column_name in enumerate(columns):\n",
    "            val = row[column_name]\n",
    "            if val > 0:\n",
    "                csr_rows.append(row_idx)\n",
    "                csr_cols.append(col_idx)\n",
    "                csr_values.append(val)\n",
    "            # if row[column_name] is not None:\n",
    "            #     assert row[column_name] in values, f\"Unexpected value {row[column_name]} in column {column_name}\"\n",
    "    # for _, row in batch:\n",
    "    #     print(row)\n",
    "    #     break\n",
    "\n",
    "print(\"CSR size:\", len(csr_values), \"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(csr_cols), max(csr_rows), len(data), len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "n_rows = count_embeddings.rows\n",
    "n_cols = len(count_embeddings.columns)\n",
    "coo = sp.coo_matrix((csr_values, (csr_rows, csr_cols)), shape=(n_rows, n_cols))\n",
    "csr = coo.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Sparse memory (CSR)\n",
    "csr_mem = csr.data.nbytes + csr.indices.nbytes + csr.indptr.nbytes\n",
    "print(\"CSR matrix size:\", csr_mem / 1e6, \"MB\")\n",
    "\n",
    "# 3️⃣ Sparse memory (COO)\n",
    "coo_mem = coo.data.nbytes + coo.row.nbytes + coo.col.nbytes\n",
    "print(\"COO matrix size:\", coo_mem / 1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    csr,\n",
    "    index=count_embeddings.to_duckdb().select(\"region_id\").fetchnumpy()[\"region_id\"],\n",
    "    columns=count_embeddings.columns,\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "arrow_sparse_csr_matrix = pa.SparseCSRMatrix.from_scipy(csr)\n",
    "arrow_sparse_csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = count_embeddings.to_dataframe()\n",
    "\n",
    "print(\n",
    "    \"DF dense size:\",\n",
    "    (ddf.index.nbytes + ddf.columns.nbytes + sum(ddf[c].values.nbytes for c in ddf.columns)) / 1e6,\n",
    "    \"MB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sparse: {ddf.memory_usage().sum() / 1e6:0.2f} mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sparse: {df.memory_usage().sum() / 1e6:0.2f} mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.join(df, rsuffix=\"_sparse\")  # .memory_usage().sum() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"DF sparse size:\",\n",
    "    (df.index.nbytes + df.columns.nbytes + sum(df[c].values.nbytes for c in df.columns)) / 1e6,\n",
    "    \"MB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "values = []\n",
    "\n",
    "count_embeddings.to_dataframe().values\n",
    "\n",
    "# Imagine you iterate over your data rows\n",
    "for row_idx, (region, feature_dict) in enumerate(data):\n",
    "    # feature_dict = {col_idx: value, ...}, e.g. {12: 0.7, 1134: 0.2}\n",
    "    for col_idx, val in feature_dict.items():\n",
    "        if val != 0:\n",
    "            rows.append(row_idx)\n",
    "            cols.append(col_idx)\n",
    "            values.append(val)\n",
    "\n",
    "# Build sparse matrix once\n",
    "n_rows = len(data)\n",
    "n_cols = 5000  # number of feature columns\n",
    "coo = sp.coo_matrix((values, (rows, cols)), shape=(n_rows, n_cols))\n",
    "csr = coo.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = count_embeddings.to_duckdb()\n",
    "rel.columns[1:]\n",
    "\n",
    "\n",
    "cols = list(map(lambda c: f'\"{c}\"', rel.columns[1:4346]))\n",
    "\n",
    "print(f\"\"\"\n",
    "    SELECT x.region_id, {\", \".join(cols)}\n",
    "    FROM ({rel.sql_query()}) x\n",
    "    \"\"\")\n",
    "\n",
    "r = duckdb.sql(\n",
    "    f\"\"\"\n",
    "    SELECT region_id, array_value({\", \".join(cols)}) AS embedding\n",
    "    FROM read_parquet('files/pdt/CountEmbedder_20251005_132917_045489_embeddings/20251005_132917_049375.parquet')\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# rel.to_parquet('column_embeddings.parquet')\n",
    "# r.to_parquet('array_embeddings.parquet')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_parquet(\"array_embeddings.parquet\").pl().cast(\n",
    "    {\"embedding\": pl.Array(pl.Int32, shape=73)}\n",
    ").head(2).select(pl.col(\"embedding\")).sum()  # .group_by(\"region_id\").agg(pl.col(\"embedding\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet(\"column_embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet(\"array_embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet(\n",
    "    \"array_embeddings.parquet\",\n",
    "    schema={\"region_id\": pl.UInt64, \"embedding\": pl.Array(pl.Int32, shape=73)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_df = count_embeddings.to_duckdb().pl()\n",
    "cols = pl_df.columns[1:]\n",
    "pl_df.select(pl.str)\n",
    "# pl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\n",
    "    \"\"\"\n",
    "    WITH arrays as (\n",
    "        SELECT UNNEST([\n",
    "            [1, 5, 7],\n",
    "            [4, 6, 1],\n",
    "            [6, 0, 7],\n",
    "            [5, 3, 2]\n",
    "        ]) as values\n",
    "    )\n",
    "    SELECT SUM(values) FROM arrays\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srai-3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
